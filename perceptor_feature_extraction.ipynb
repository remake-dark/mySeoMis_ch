{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc6849b7-21ee-479a-b059-595c3cee6281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at ./chinese-bert-wwm-ext were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1439\n",
      "1438\n",
      "6713\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import BertForMaskedLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. 加载模型和分词器\n",
    "model_path = './chinese-bert-wwm-ext'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForMaskedLM.from_pretrained(model_path, output_hidden_states=True)\n",
    "\n",
    "# 假设你的JSON文件路径是'your_file_path.json'\n",
    "file_path = './data/train.json'\n",
    "# 使用pandas的read_json函数读取JSON文件\n",
    "df_train = pd.read_json(file_path,orient='records', lines=True)\n",
    "train_len = df_train.shape[0]\n",
    "\n",
    "file_path_val = './data/val.json'\n",
    "\n",
    "# 使用pandas的read_json函数读取JSON文件\n",
    "df_val = pd.read_json(file_path_val,orient='records', lines=True)\n",
    "val_len = df_val.shape[0]\n",
    "\n",
    "file_path_test = './data/test.json'\n",
    "\n",
    "# 使用pandas的read_json函数读取JSON文件\n",
    "df_test = pd.read_json(file_path_test,orient='records', lines=True)\n",
    "test_len = df_test.shape[0]\n",
    "df = pd.concat([df_train,df_val, df_test])\n",
    "print(test_len)\n",
    "print(val_len)\n",
    "print(train_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "310ad076-3592-44e3-b9ac-85480b8a7f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['灾难事故', '社会生活', '政治', '医药健康', '文体娱乐', '军事', '科技', '财经商业', '教育考试',\n",
       "       '无法确定'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.domain.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba39b8bd-500e-40e7-bb4e-b144139710f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.rename(columns={'event': 'entity_list'}, inplace=True)\n",
    "df_val.rename(columns={'event': 'entity_list'}, inplace=True)\n",
    "df_test.rename(columns={'event': 'entity_list'}, inplace=True)\n",
    "df.rename(columns={'event': 'entity_list'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8914592a-f408-411f-9b44-5a4557e5a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('./data/all_data.json',orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee4cd23-98a6-405c-9b82-fd5152341b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_json('./data/train.json',orient='records', lines=True)\n",
    "df_val.to_json('./data/val.json',orient='records', lines=True)\n",
    "df_test.to_json('./data/test.json',orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8db18d6-fe83-4aee-9c87-d71ce6df521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.load('./data/soc_feat_w_bert.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd9b1ec0-c66b-4237-843c-1d17f763dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_test = arr[19314-4733:,:]\n",
    "arr_test.shape\n",
    "np.save('./data/soc_feat_w_bert_test.npy', arr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f5db9f2-574e-400e-989b-9590a3a46850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "file_path = './data/train.json'\n",
    "# 使用pandas的read_json函数读取JSON文件\n",
    "df_train = pd.read_json(file_path,orient='records', lines=True)\n",
    "train_len = df_train.shape[0]\n",
    "#df_train = df_train.rename(columns={'event': 'entity_list'})\n",
    "file_path_val = './data/val.json'\n",
    "\n",
    "# 使用pandas的read_json函数读取JSON文件\n",
    "df_val = pd.read_json(file_path_val,orient='records', lines=True)\n",
    "val_len = df_val.shape[0]\n",
    "#df_val = df_val.rename(columns={'event': 'entity_list'})\n",
    "file_path_test = './data/test.json'\n",
    "\n",
    "# 使用pandas的read_json函数读取JSON文件\n",
    "df_test = pd.read_json(file_path_test,orient='records', lines=True)\n",
    "#df_test = df_test.rename(columns={'event': 'entity_list'})\n",
    "test_len = df_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aaf083f-ea27-4eb4-9613-bed91373b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.loc[df_train['label'] == 10, 'label'] = 0\n",
    "\n",
    "# 将df_train中label列的值从11改为1\n",
    "df_test.loc[df_val['label'] == 11, 'label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9a25251-675c-4a9b-b483-c4be54376d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1875\n",
       "1    1625\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b9e821-b1b3-46f2-8db8-6e7291016378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.to_json('./data/train.json',orient='records', lines=True)\n",
    "df_val.to_json('./data/val.json',orient='records', lines=True)\n",
    "#df_test.to_json('./data/test.json',orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491195e0-1611-4b8f-99ea-ba2db3d7b9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c7b5f8-6330-434c-8faa-6c36a1edcd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "file_path = './data/train.json'\n",
    "# 使用pandas的read_json函数读取JSON文件\n",
    "df_train = pd.read_json(file_path,orient='records', lines=True)\n",
    "train_len = df_train.shape[0]\n",
    "df_train = df_train.rename(columns={'event': 'entity_list'})\n",
    "file_path_val = './data/val.json'\n",
    "\n",
    "# 使用pandas的read_json函数读取JSON文件\n",
    "df_val = pd.read_json(file_path_val,orient='records', lines=True)\n",
    "val_len = df_val.shape[0]\n",
    "df_val = df_val.rename(columns={'event': 'entity_list'})\n",
    "file_path_test = './data/test.json'\n",
    "\n",
    "# 使用pandas的read_json函数读取JSON文件\n",
    "df_test = pd.read_json(file_path_test,orient='records', lines=True)\n",
    "df_test = df_test.rename(columns={'event': 'entity_list'})\n",
    "test_len = df_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "381a59d5-c1ed-4024-9758-184397d9f739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] 【类型】言语辱骂【/类型】【道歉对象】家里人【/道歉对象】【道歉者】我【/道歉者】【触发词】骂人【/触发词】。 [SEP]山东[SEP]15[SEP]肺炎[SEP]十五[SEP]戴口罩[SEP]怒[SEP]怒']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.iloc[11][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f628d881-5637-48b7-821b-83349dece4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_json('./data/train.json',orient='records', lines=True)\n",
    "df_val.to_json('./data/val.json',orient='records', lines=True)\n",
    "df_test.to_json('./data/test.json',orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec934b2-3e34-4042-912b-daff0bf88e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd3620c-13be-4506-af35-ac7197831b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56f05ce2-4098-4d03-b0e0-ea320bb7fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "path = './data/test.json'\n",
    "data_list = []\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "        data_list = [json.loads(line) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e8349df-fe30-4cc2-bbe5-5357c6e3bd49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '瑟瑟发抖为什么要用它的尸体泡汤喝…',\n",
       " 'label': 0,\n",
       " 'domain': 'daily',\n",
       " 'event': [' [SEP] ']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac2245bb-c0f3-4913-9e3b-474ee5a9a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "df_train = pd.read_csv('./data/train_zh.csv')\n",
    "df_val = pd.read_csv('./data/val_zh.csv')\n",
    "df_test = pd.read_csv('./data/test_zh.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "560184ad-22b2-48da-bb6e-3eece237ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['domain'] = 'daily'\n",
    "df_val['domain'] = 'daily'\n",
    "df_test['domain'] = 'daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0493fc7-1c65-4115-a642-a93ae151d8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2743/3312134979.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['domain'][:6283] = 'covid'\n",
      "/tmp/ipykernel_2743/3312134979.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['domain'][6283:6283+1899] = 'huawei'\n",
      "/tmp/ipykernel_2743/3312134979.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['domain'][6283+1899:6283+3798] = 'tradewar'\n",
      "/tmp/ipykernel_2743/3312134979.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_val['domain'][:800] = 'covid'\n",
      "/tmp/ipykernel_2743/3312134979.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_val['domain'][800:1150] = 'huawei'\n",
      "/tmp/ipykernel_2743/3312134979.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_val['domain'][1150:1500] = 'tradewar'\n"
     ]
    }
   ],
   "source": [
    "df_train['domain'][:6283] = 'covid'\n",
    "df_train['domain'][6283:6283+1899] = 'huawei'\n",
    "df_train['domain'][6283+1899:6283+3798] = 'tradewar'\n",
    "df_val['domain'][:800] = 'covid'\n",
    "df_val['domain'][800:1150] = 'huawei'\n",
    "df_val['domain'][1150:1500] = 'tradewar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69032e46-d241-4748-9d6c-744dc637451e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>天使</td>\n",
       "      <td>1</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>致敬《心》《心》小凡也要做好防护措施哦致敬《心》大家出门记得戴口罩</td>\n",
       "      <td>1</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>《中国赞》《中国赞》《中国赞》</td>\n",
       "      <td>1</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>悲壮</td>\n",
       "      <td>0</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>!!!一定会好起来</td>\n",
       "      <td>1</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>好《泪》你也要戴好口罩噢#武汉加油# 我们一起打赢这场防疫战</td>\n",
       "      <td>1</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>我妈在这种时候发烧我的天我怕死了02</td>\n",
       "      <td>0</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>《心》#致敬疫情前线医护人员# 致敬英雄们,请保护好自己《心》《心》《心》《心》《心》敬疫情...</td>\n",
       "      <td>1</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>一直都在!会好的!</td>\n",
       "      <td>1</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>#新型冠状病毒怕酒精不耐高温# 封城也没关系,坚信我大武汉一定能渡过这次难关! http:/...</td>\n",
       "      <td>1</td>\n",
       "      <td>covid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content  label domain\n",
       "0                                                  天使      1  covid\n",
       "1                   致敬《心》《心》小凡也要做好防护措施哦致敬《心》大家出门记得戴口罩      1  covid\n",
       "2                                     《中国赞》《中国赞》《中国赞》      1  covid\n",
       "3                                                  悲壮      0  covid\n",
       "4                                           !!!一定会好起来      1  covid\n",
       "..                                                ...    ...    ...\n",
       "75                     好《泪》你也要戴好口罩噢#武汉加油# 我们一起打赢这场防疫战      1  covid\n",
       "76                                 我妈在这种时候发烧我的天我怕死了02      0  covid\n",
       "77  《心》#致敬疫情前线医护人员# 致敬英雄们,请保护好自己《心》《心》《心》《心》《心》敬疫情...      1  covid\n",
       "78                                          一直都在!会好的!      1  covid\n",
       "79  #新型冠状病毒怕酒精不耐高温# 封城也没关系,坚信我大武汉一定能渡过这次难关! http:/...      1  covid\n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7964fbc4-3e73-46cb-a0fb-6f964ea6758e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7728, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb5e9bc-cf31-4f40-908a-1d4f76b1f8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aef20d9-394d-44ca-b3de-6fe04b71933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "path = './data/val.json'\n",
    "with open(path, 'r', encoding='utf-8') as file:\n",
    "    data_list = [json.loads(line) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8292ef83-e1d0-4732-9a74-e46dc1c57a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dear @skynews pls tweet @skystephen ' excellent summaries of hurricane irma as # geography teachers / students can use these excellent summaries\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ec8c77c5-0408-478d-9eef-7ca67e17b2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7728"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db7fa582-6494-47fb-a3bb-9fa4b9843d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_json('./data/train.json',orient='records', lines=True)\n",
    "df_val.to_json('./data/val.json',orient='records', lines=True)\n",
    "df_test.to_json('./data/test.json',orient='records', lines=True)\n",
    "#df.to_json('./data/all_data.json',orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c76866a-4ef2-4227-8b3b-2af6625c8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('./data/all_data.json',orient='records',lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50092446-1008-4bb9-ae1b-a31bdfabd872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [Type] attack [/Type][Place] dominica [/Place][Trigger] ravaged [/Trigger]. [SEP] || [CLS] [Type] transfermoney [/Type][Agent] venezuela [/Agent][Artifact] aid [/Artifact][Destination] dominica [/Destination][Trigger] sending [/Trigger]. [SEP]POTUS Venezuela [SEP] Hurricane [SEP] Dominica'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[0][2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3faba549-1591-4d6b-b150-ff15ee003690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The South Houston [SEP] industrial area [SEP] Rapkin [SEP] SMP'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc [1][2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52c73062-ae6f-412a-b5d0-58542316dcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [SEP] '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[9000][2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "935dc6df-c1f6-4be1-b42c-5066e5024fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [Type] attack [/Type][target] reporter [/target][attacker] irma [/attacker][Trigger] blow [/Trigger]. [SEP]Hurricane Irma [SEP] reporter'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.iloc[18][2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8dff72a-0c5d-46fb-a28a-6822236b42c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the hurricane harvey trailer is 3/4 of the way...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[CLS] [Type] transport [/Type][Destination] l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>at least 22 killed in cuba after hurricane irm...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[CLS] [Type] attack [/Type][Place] cuba [/Pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>check this out to draw distinctions between do...</td>\n",
       "      <td>1</td>\n",
       "      <td>[drones]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>miranda lambert , chris stapleton and more are...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Hurricane Harvey [SEP] music]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>please donate to the american red cross to hel...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[CLS] [Type] transfermoney [/Type][Trigger] d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  label  \\\n",
       "0  the hurricane harvey trailer is 3/4 of the way...      1   \n",
       "1  at least 22 killed in cuba after hurricane irm...      1   \n",
       "2  check this out to draw distinctions between do...      1   \n",
       "3  miranda lambert , chris stapleton and more are...      1   \n",
       "4  please donate to the american red cross to hel...      1   \n",
       "\n",
       "                                               event  \n",
       "0  [[CLS] [Type] transport [/Type][Destination] l...  \n",
       "1  [[CLS] [Type] attack [/Type][Place] cuba [/Pla...  \n",
       "2                                           [drones]  \n",
       "3                     [Hurricane Harvey [SEP] music]  \n",
       "4  [[CLS] [Type] transfermoney [/Type][Trigger] d...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23d5ea05-d952-4782-8dfb-f438129968c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POTUS Venezuela', 'Hurricane', 'Dominica']\n",
      "[['POTUS Venezuela', 'Hurricane', 'Dominica'], ['Hurricane Irma', 'reporter'], ['Irma'], []]\n"
     ]
    }
   ],
   "source": [
    "def extract_entities(data):\n",
    "    def parse_entities(text):\n",
    "        # 查找最后一个 \". [SEP]\" 的位置\n",
    "        last_sep_index = text.rfind('. [SEP]')\n",
    "        # 根据找到的位置提取后面的所有文本\n",
    "        if last_sep_index != -1:\n",
    "            entities_part = text[last_sep_index + len('. [SEP]'):]\n",
    "        else:\n",
    "            entities_part = text\n",
    "        \n",
    "        # 分割字符串获取实体列表\n",
    "        entities = entities_part.split(' [SEP] ')\n",
    "        \n",
    "        # 移除空字符串实体\n",
    "        entities = [entity for entity in entities if entity.strip()]\n",
    "        \n",
    "        return entities\n",
    "\n",
    "    # 判断输入类型是单个字符串还是列表\n",
    "    if isinstance(data, list):\n",
    "        # 如果是列表，则对列表中每个字符串项调用 parse_entities\n",
    "        return [parse_entities(item) for item in data]\n",
    "    else:\n",
    "        # 如果是单个字符串，直接处理这个字符串\n",
    "        return parse_entities(data)\n",
    "\n",
    "# 示例数据\n",
    "samples = [\n",
    "    '[CLS] [Type] attack [/Type][Place] dominica [/Place][Trigger] ravaged [/Trigger]. [SEP] || [CLS] [Type] transfermoney [/Type][Agent] venezuela [/Agent][Artifact] aid [/Artifact][Destination] dominica [/Destination][Trigger] sending [/Trigger]. [SEP]POTUS Venezuela [SEP] Hurricane [SEP] Dominica',\n",
    "    '[CLS] [Type] attack [/Type][target] reporter [/target][attacker] irma [/attacker][Trigger] blow [/Trigger]. [SEP]Hurricane Irma [SEP] reporter',\n",
    "    'Irma',\n",
    "    ' [SEP] '\n",
    "]\n",
    "\n",
    "# 测试函数，单个字符串输入\n",
    "print(extract_entities(samples[0]))\n",
    "\n",
    "# 测试函数，字符串列表输入\n",
    "print(extract_entities(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "828ff120-ca41-446b-b9c1-e90fb34f5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "def generate_random_string(length):\n",
    "    \"\"\"生成一个指定长度的随机字符串，包含大小写字母\"\"\"\n",
    "    letters = string.ascii_letters\n",
    "    return ''.join(random.choice(letters) for _ in range(length))\n",
    "\n",
    "def create_sample():\n",
    "    \"\"\"创建一个模拟的数据样本，格式类似之前的描述\"\"\"\n",
    "    parts = []\n",
    "    # 随机生成0-3组事件特征\n",
    "    for _ in range(random.randint(0, 3)):\n",
    "        type_ = generate_random_string(random.randint(5, 10))\n",
    "        place = generate_random_string(random.randint(5, 10))\n",
    "        trigger = generate_random_string(random.randint(5, 10))\n",
    "        parts.append(f'[CLS] [Type] {type_} [/Type][Place] {place} [/Place][Trigger] {trigger} [/Trigger]')\n",
    "    \n",
    "    if parts:\n",
    "        parts.append('. [SEP]')\n",
    "    \n",
    "    # 随机生成1-5个实体特征\n",
    "    entity_count = random.randint(1, 5)\n",
    "    entities = ' [SEP] '.join(generate_random_string(random.randint(3, 8)) for _ in range(entity_count))\n",
    "    \n",
    "    parts.append(entities)\n",
    "    \n",
    "    return ' || '.join(parts)\n",
    "\n",
    "# 生成并输出5000个不同的测试字符串\n",
    "for i in range(5000):\n",
    "    sample = create_sample()\n",
    "    extract_entities(sample)\n",
    "    #print(f'Sample {i+1}: {sample}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ac61584-d55d-4fe4-93fa-98aa0cd32749",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = df.content.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3a91525-2d06-4f52-86f0-31a3a86a19e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content    #青岛下冰雹#辟谣，刚经过福州北路，不是立交桥塌了，也不是过街天桥塌了，是天桥旁边工地的施工...\n",
       "label                                                      0\n",
       "domain                                                  灾难事故\n",
       "event      ['[CLS] 【类型】灾害/意外坍/垮塌【/类型】【坍塌主体】天桥旁边工地的施工装置【/坍...\n",
       "Name: 4187, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[4187]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a82f7a5a-6a5c-475d-a91d-9a4b4792841c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999\n",
      "None\n",
      "4180\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lst)):\n",
    "    if not isinstance(lst[i], str):\n",
    "        print(i)\n",
    "        print(lst[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3808ad0-830b-4ce6-9435-5283d82e3fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training: 100%|██████████| 300/300 [01:36<00:00,  3.10it/s]\n",
      "Processing batches -- loading embeddings for clustering: 100%|██████████| 600/600 [00:47<00:00, 12.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer, AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1'\n",
    "output_layer_id = 11\n",
    "# 加载预训练的模型和分词器 设置为训练模式\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=170)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "model.train()\n",
    "\n",
    "dataset = TextDataset(df['content'].tolist(), tokenizer)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "batch_size = 32  # 根据GPU调整\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 训练一个epoch，包括进度条\n",
    "for batch in tqdm(dataloader, desc=\"Training\", total=len(dataloader)):\n",
    "    inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(**inputs)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "# 切换到评估模式\n",
    "model.eval()\n",
    "\n",
    "def get_cls_from_bert_layer(texts, output_layer_id, batch_size=16):\n",
    "    total = len(texts)\n",
    "    all_cls_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, total, batch_size), desc=\"Processing batches -- loading embeddings for clustering\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "        \n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        target_layer_output = outputs.hidden_states[output_layer_id]\n",
    "        cls_embeddings = target_layer_output[:, 0, :]\n",
    "        all_cls_embeddings.append(cls_embeddings.cpu())\n",
    "        \n",
    "    all_cls_embeddings = torch.cat(all_cls_embeddings, dim=0)\n",
    "    return all_cls_embeddings\n",
    "\n",
    "cls_embeddings = get_cls_from_bert_layer(df['content'].tolist(), output_layer_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3e0a42b-088e-4dc5-81a2-787077f226a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLS embeddings have been saved to 'cls_embeddings.pkl'\n"
     ]
    }
   ],
   "source": [
    "folder_path = './preprocessed'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# 现在可以安全地保存文件\n",
    "file_path = os.path.join(folder_path, 'cls_embeddings.pkl')\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(cls_embeddings, f)\n",
    "\n",
    "print(\"CLS embeddings have been saved to 'cls_embeddings.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d632d8e2-a2e6-402a-b122-73cd6226d6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e072dd77-b3c3-4c5b-8b77-23dfaee7b7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007c4812-247e-4d25-b79d-6be46c371f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9590, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b790ab0f-d75b-4dc5-8ed4-8c59b976a772",
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m     SBERT_embeddings \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m----> 9\u001b[0m     cls_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "folder_path = './preprocessed'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# 现在可以安全地保存文件\n",
    "file_path = os.path.join(folder_path, 'cls_embeddings.pkl')\n",
    "with open(file_path, 'rb') as f:\n",
    "    SBERT_embeddings = pickle.load(f)\n",
    "    cls_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02479e11-1357-4206-9f4c-21e438932175",
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls_embeddings.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 9\u001b[0m     cls_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "folder_path = './preprocessed'\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# 现在可以安全地保存文件\n",
    "file_path = os.path.join(folder_path, 'cls_embeddings.pkl')\n",
    "with open(file_path, 'rb') as f:\n",
    "    cls_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "216b4626-8314-4be6-af5b-100de314e113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9590/9590 [00:00<00:00, 257447.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np_data shape:(9590, 768) np_data type:<class 'numpy.ndarray'>\n",
      "Loaded embeddings: [[-1.5623671e-01 -2.5833985e-01 -1.0880558e-01 ...  1.8318647e-01\n",
      "   1.0182659e-01 -5.8643574e-01]\n",
      " [ 6.7276132e-01 -1.6450267e-01  8.9874953e-01 ... -5.2653130e-02\n",
      "   1.2397858e-02 -7.1416181e-01]\n",
      " [-5.2378416e-02  2.9921240e-01  4.2665318e-02 ...  3.0002162e-01\n",
      "  -5.3610063e-01 -4.2368615e-01]\n",
      " ...\n",
      " [ 7.8243029e-04  8.5165076e-02  6.9026068e-02 ...  2.4559766e-01\n",
      "  -7.7260174e-02 -3.0785292e-01]\n",
      " [-2.3963002e-02  9.7701974e-02  5.0742346e-01 ...  2.0365345e-01\n",
      "  -5.8047849e-01 -1.0005008e+00]\n",
      " [ 2.8672728e-01  2.5062391e-01  8.3255225e-01 ...  1.9413692e-01\n",
      "   1.5077128e-01 -1.4123702e-01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def load_SBERT_embeddings(embedding_path):\n",
    "    print('Loading BERT embeddings...')\n",
    "    with open(embedding_path, 'rb') as handle:\n",
    "        pkl_data = pickle.load(handle)\n",
    "\n",
    "    # 将数据从字典转换成 NumPy 数组\n",
    "    np_data = []\n",
    "    if torch.is_tensor(pkl_data):\n",
    "        for val in tqdm(pkl_data):\n",
    "            np_data.append(val.numpy())\n",
    "    else:\n",
    "        for i, val in pkl_data.items():\n",
    "            np_data.append(val)\n",
    "    np_data = np.array(np_data)\n",
    "    print('np_data shape:{} np_data type:{}'.format(np_data.shape, type(np_data)))\n",
    "\n",
    "    return np_data\n",
    "\n",
    "# 示例用法\n",
    "embedding_path = file_path  # 替换为实际的文件路径\n",
    "embeddings = load_SBERT_embeddings(embedding_path)\n",
    "print('Loaded embeddings:', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bbcd40e-9ce6-46d4-a1cb-10b4030a0b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9590, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SBERT_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c39542a5-4f43-4c5d-ae8d-7fab52667173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import jieba\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04b48e6f-1797-4357-94f4-96470f52811c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_name': 'train', 'embedding_type': 'SBERT', 'cluster_threshold': 0.78, 'train_data_path': './data/all_data.json', 'user_save_dir': './preprocessed/', 'SBERT_embeddings_path': './preprocessed/cls_embeddings.pkl', 'thres_low': 0, 'thres_high': inf}\n",
      "========== single pass cluster ==========\n",
      "dataset_name: train\n",
      "{'data_dirs': './data/', 'train_data_paths': './data/all_data.json', 'dataset_name': 'train', 'user_save_dirs': './preprocessed/', 'cluster_name': 'SBERT_0.78'}\n",
      "============================================================\n",
      "==================== embedding_type: SBERT ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- threshold: 0.78 ----------\n",
      "load in BERT embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 9590/9590 [00:00<00:00, 269099.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np_data shape:(9590, 768) np_data type:<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "183it [00:00, 1827.53it/s]\u001b[A\n",
      "366it [00:00, 1332.82it/s]\u001b[A\n",
      "508it [00:00, 1073.70it/s]\u001b[A\n",
      "623it [00:00, 955.29it/s] \u001b[A\n",
      "723it [00:00, 872.14it/s]\u001b[A\n",
      "813it [00:00, 809.93it/s]\u001b[A\n",
      "896it [00:00, 762.55it/s]\u001b[A\n",
      "973it [00:01, 725.94it/s]\u001b[A\n",
      "1046it [00:01, 691.84it/s]\u001b[A\n",
      "1115it [00:01, 657.03it/s]\u001b[A\n",
      "1181it [00:01, 601.06it/s]\u001b[A\n",
      "1242it [00:01, 563.99it/s]\u001b[A\n",
      "1299it [00:01, 532.54it/s]\u001b[A\n",
      "1353it [00:01, 507.97it/s]\u001b[A\n",
      "1404it [00:01, 491.47it/s]\u001b[A\n",
      "1453it [00:02, 474.68it/s]\u001b[A\n",
      "1501it [00:02, 462.00it/s]\u001b[A\n",
      "1547it [00:02, 449.67it/s]\u001b[A\n",
      "1592it [00:02, 440.28it/s]\u001b[A\n",
      "1636it [00:02, 429.81it/s]\u001b[A\n",
      "1679it [00:02, 423.05it/s]\u001b[A\n",
      "1722it [00:02, 416.97it/s]\u001b[A\n",
      "1764it [00:02, 411.32it/s]\u001b[A\n",
      "1806it [00:02, 406.13it/s]\u001b[A\n",
      "1847it [00:03, 398.71it/s]\u001b[A\n",
      "1887it [00:03, 396.47it/s]\u001b[A\n",
      "1927it [00:03, 389.23it/s]\u001b[A\n",
      "1966it [00:03, 388.78it/s]\u001b[A\n",
      "2005it [00:03, 374.66it/s]\u001b[A\n",
      "2043it [00:03, 374.51it/s]\u001b[A\n",
      "2081it [00:03, 369.23it/s]\u001b[A\n",
      "2118it [00:03, 368.76it/s]\u001b[A\n",
      "2155it [00:03, 365.32it/s]\u001b[A\n",
      "2192it [00:03, 365.02it/s]\u001b[A\n",
      "2229it [00:04, 365.23it/s]\u001b[A\n",
      "2266it [00:04, 353.98it/s]\u001b[A\n",
      "2302it [00:04, 339.98it/s]\u001b[A\n",
      "2337it [00:04, 329.19it/s]\u001b[A\n",
      "2371it [00:04, 318.51it/s]\u001b[A\n",
      "2403it [00:04, 314.11it/s]\u001b[A\n",
      "2435it [00:04, 306.29it/s]\u001b[A\n",
      "2466it [00:04, 300.56it/s]\u001b[A\n",
      "2497it [00:04, 299.92it/s]\u001b[A\n",
      "2528it [00:05, 295.60it/s]\u001b[A\n",
      "2558it [00:05, 293.18it/s]\u001b[A\n",
      "2588it [00:05, 291.26it/s]\u001b[A\n",
      "2618it [00:05, 288.37it/s]\u001b[A\n",
      "2647it [00:05, 285.06it/s]\u001b[A\n",
      "2676it [00:05, 284.14it/s]\u001b[A\n",
      "2705it [00:05, 283.06it/s]\u001b[A\n",
      "2734it [00:05, 279.20it/s]\u001b[A\n",
      "2762it [00:05, 279.13it/s]\u001b[A\n",
      "2790it [00:06, 278.39it/s]\u001b[A\n",
      "2820it [00:06, 282.21it/s]\u001b[A\n",
      "2849it [00:06, 278.39it/s]\u001b[A\n",
      "2877it [00:06, 275.18it/s]\u001b[A\n",
      "2905it [00:06, 274.06it/s]\u001b[A\n",
      "2933it [00:06, 274.63it/s]\u001b[A\n",
      "2962it [00:06, 277.39it/s]\u001b[A\n",
      "2990it [00:06, 274.15it/s]\u001b[A\n",
      "3018it [00:06, 274.64it/s]\u001b[A\n",
      "3046it [00:06, 271.96it/s]\u001b[A\n",
      "3074it [00:07, 270.21it/s]\u001b[A\n",
      "3102it [00:07, 272.67it/s]\u001b[A\n",
      "3131it [00:07, 277.32it/s]\u001b[A\n",
      "3159it [00:07, 273.13it/s]\u001b[A\n",
      "3187it [00:07, 268.41it/s]\u001b[A\n",
      "3214it [00:07, 265.08it/s]\u001b[A\n",
      "3241it [00:07, 265.15it/s]\u001b[A\n",
      "3268it [00:07, 256.96it/s]\u001b[A\n",
      "3297it [00:07, 264.86it/s]\u001b[A\n",
      "3324it [00:08, 260.33it/s]\u001b[A\n",
      "3351it [00:08, 250.53it/s]\u001b[A\n",
      "3378it [00:08, 255.89it/s]\u001b[A\n",
      "3404it [00:08, 252.16it/s]\u001b[A\n",
      "3430it [00:08, 238.97it/s]\u001b[A\n",
      "3455it [00:08, 238.51it/s]\u001b[A\n",
      "3482it [00:08, 244.65it/s]\u001b[A\n",
      "3509it [00:08, 250.05it/s]\u001b[A\n",
      "3535it [00:08, 248.15it/s]\u001b[A\n",
      "3562it [00:08, 253.25it/s]\u001b[A\n",
      "3588it [00:09, 251.16it/s]\u001b[A\n",
      "3614it [00:09, 252.09it/s]\u001b[A\n",
      "3640it [00:09, 252.13it/s]\u001b[A\n",
      "3667it [00:09, 255.22it/s]\u001b[A\n",
      "3695it [00:09, 261.01it/s]\u001b[A\n",
      "3722it [00:09, 252.14it/s]\u001b[A\n",
      "3748it [00:09, 251.98it/s]\u001b[A\n",
      "3774it [00:09, 250.64it/s]\u001b[A\n",
      "3801it [00:09, 254.91it/s]\u001b[A\n",
      "3827it [00:10, 252.26it/s]\u001b[A\n",
      "3853it [00:10, 248.46it/s]\u001b[A\n",
      "3879it [00:10, 249.95it/s]\u001b[A\n",
      "3905it [00:10, 247.06it/s]\u001b[A\n",
      "3931it [00:10, 248.98it/s]\u001b[A\n",
      "3957it [00:10, 251.36it/s]\u001b[A\n",
      "3983it [00:10, 250.71it/s]\u001b[A\n",
      "4009it [00:10, 243.55it/s]\u001b[A\n",
      "4035it [00:10, 247.12it/s]\u001b[A\n",
      "4061it [00:10, 250.65it/s]\u001b[A\n",
      "4087it [00:11, 244.98it/s]\u001b[A\n",
      "4112it [00:11, 245.36it/s]\u001b[A\n",
      "4137it [00:11, 242.13it/s]\u001b[A\n",
      "4162it [00:11, 237.61it/s]\u001b[A\n",
      "4186it [00:11, 233.44it/s]\u001b[A\n",
      "4211it [00:11, 237.75it/s]\u001b[A\n",
      "4235it [00:11, 233.11it/s]\u001b[A\n",
      "4259it [00:11, 233.40it/s]\u001b[A\n",
      "4283it [00:11, 232.13it/s]\u001b[A\n",
      "4307it [00:12, 229.16it/s]\u001b[A\n",
      "4331it [00:12, 231.90it/s]\u001b[A\n",
      "4356it [00:12, 234.82it/s]\u001b[A\n",
      "4380it [00:12, 227.93it/s]\u001b[A\n",
      "4403it [00:12, 227.76it/s]\u001b[A\n",
      "4429it [00:12, 236.67it/s]\u001b[A\n",
      "4453it [00:12, 232.45it/s]\u001b[A\n",
      "4477it [00:12, 229.15it/s]\u001b[A\n",
      "4505it [00:12, 241.03it/s]\u001b[A\n",
      "4530it [00:12, 240.87it/s]\u001b[A\n",
      "4555it [00:13, 241.02it/s]\u001b[A\n",
      "4582it [00:13, 248.77it/s]\u001b[A\n",
      "4609it [00:13, 253.04it/s]\u001b[A\n",
      "4637it [00:13, 257.11it/s]\u001b[A\n",
      "4663it [00:13, 256.67it/s]\u001b[A\n",
      "4689it [00:13, 252.58it/s]\u001b[A\n",
      "4715it [00:13, 244.95it/s]\u001b[A\n",
      "4745it [00:13, 259.97it/s]\u001b[A\n",
      "4773it [00:13, 263.20it/s]\u001b[A\n",
      "4801it [00:14, 266.96it/s]\u001b[A\n",
      "4828it [00:14, 264.35it/s]\u001b[A\n",
      "4855it [00:14, 255.94it/s]\u001b[A\n",
      "4882it [00:14, 256.79it/s]\u001b[A\n",
      "4908it [00:14, 246.74it/s]\u001b[A\n",
      "4934it [00:14, 249.07it/s]\u001b[A\n",
      "4962it [00:14, 257.66it/s]\u001b[A\n",
      "4989it [00:14, 260.00it/s]\u001b[A\n",
      "5016it [00:14, 245.84it/s]\u001b[A\n",
      "5043it [00:14, 251.78it/s]\u001b[A\n",
      "5069it [00:15, 251.06it/s]\u001b[A\n",
      "5095it [00:15, 252.85it/s]\u001b[A\n",
      "5121it [00:15, 247.67it/s]\u001b[A\n",
      "5146it [00:15, 243.84it/s]\u001b[A\n",
      "5171it [00:15, 238.98it/s]\u001b[A\n",
      "5196it [00:15, 239.26it/s]\u001b[A\n",
      "5221it [00:15, 241.59it/s]\u001b[A\n",
      "5251it [00:15, 257.39it/s]\u001b[A\n",
      "5277it [00:15, 250.89it/s]\u001b[A\n",
      "5303it [00:16, 252.61it/s]\u001b[A\n",
      "5329it [00:16, 248.68it/s]\u001b[A\n",
      "5354it [00:16, 240.35it/s]\u001b[A\n",
      "5379it [00:16, 239.13it/s]\u001b[A\n",
      "5403it [00:16, 229.26it/s]\u001b[A\n",
      "5427it [00:16, 227.20it/s]\u001b[A\n",
      "5452it [00:16, 230.39it/s]\u001b[A\n",
      "5477it [00:16, 234.95it/s]\u001b[A\n",
      "5501it [00:16, 234.43it/s]\u001b[A\n",
      "5525it [00:16, 231.49it/s]\u001b[A\n",
      "5553it [00:17, 242.37it/s]\u001b[A\n",
      "5578it [00:17, 242.03it/s]\u001b[A\n",
      "5603it [00:17, 229.21it/s]\u001b[A\n",
      "5628it [00:17, 233.55it/s]\u001b[A\n",
      "5653it [00:17, 237.09it/s]\u001b[A\n",
      "5680it [00:17, 244.54it/s]\u001b[A\n",
      "5705it [00:17, 225.13it/s]\u001b[A\n",
      "5730it [00:17, 231.47it/s]\u001b[A\n",
      "5757it [00:17, 241.99it/s]\u001b[A\n",
      "5785it [00:18, 251.23it/s]\u001b[A\n",
      "5811it [00:18, 239.24it/s]\u001b[A\n",
      "5836it [00:18, 224.69it/s]\u001b[A\n",
      "5865it [00:18, 239.53it/s]\u001b[A\n",
      "5890it [00:18, 239.08it/s]\u001b[A\n",
      "5915it [00:18, 241.56it/s]\u001b[A\n",
      "5940it [00:18, 231.51it/s]\u001b[A\n",
      "5965it [00:18, 235.82it/s]\u001b[A\n",
      "5989it [00:18, 226.86it/s]\u001b[A\n",
      "6013it [00:19, 229.68it/s]\u001b[A\n",
      "6037it [00:19, 231.59it/s]\u001b[A\n",
      "6061it [00:19, 217.31it/s]\u001b[A\n",
      "6088it [00:19, 229.76it/s]\u001b[A\n",
      "6112it [00:19, 223.61it/s]\u001b[A\n",
      "6135it [00:19, 221.72it/s]\u001b[A\n",
      "6160it [00:19, 225.84it/s]\u001b[A\n",
      "6185it [00:19, 232.20it/s]\u001b[A\n",
      "6209it [00:19, 227.59it/s]\u001b[A\n",
      "6234it [00:20, 230.77it/s]\u001b[A\n",
      "6258it [00:20, 229.02it/s]\u001b[A\n",
      "6281it [00:20, 219.49it/s]\u001b[A\n",
      "6304it [00:20, 214.28it/s]\u001b[A\n",
      "6326it [00:20, 212.25it/s]\u001b[A\n",
      "6349it [00:20, 216.16it/s]\u001b[A\n",
      "6371it [00:20, 212.40it/s]\u001b[A\n",
      "6395it [00:20, 217.96it/s]\u001b[A\n",
      "6419it [00:20, 223.65it/s]\u001b[A\n",
      "6445it [00:20, 232.66it/s]\u001b[A\n",
      "6469it [00:21, 223.85it/s]\u001b[A\n",
      "6492it [00:21, 216.23it/s]\u001b[A\n",
      "6514it [00:21, 212.21it/s]\u001b[A\n",
      "6538it [00:21, 218.78it/s]\u001b[A\n",
      "6560it [00:21, 215.22it/s]\u001b[A\n",
      "6582it [00:21, 207.60it/s]\u001b[A\n",
      "6603it [00:21, 198.94it/s]\u001b[A\n",
      "6625it [00:21, 201.90it/s]\u001b[A\n",
      "6646it [00:21, 198.55it/s]\u001b[A\n",
      "6666it [00:22, 190.49it/s]\u001b[A\n",
      "6690it [00:22, 201.19it/s]\u001b[A\n",
      "6711it [00:22, 202.10it/s]\u001b[A\n",
      "6733it [00:22, 204.06it/s]\u001b[A\n",
      "6754it [00:22, 197.91it/s]\u001b[A\n",
      "6777it [00:22, 206.61it/s]\u001b[A\n",
      "6800it [00:22, 212.87it/s]\u001b[A\n",
      "6822it [00:22, 211.58it/s]\u001b[A\n",
      "6844it [00:22, 205.48it/s]\u001b[A\n",
      "6865it [00:23, 206.30it/s]\u001b[A\n",
      "6887it [00:23, 210.20it/s]\u001b[A\n",
      "6909it [00:23, 209.60it/s]\u001b[A\n",
      "6931it [00:23, 203.61it/s]\u001b[A\n",
      "6952it [00:23, 199.19it/s]\u001b[A\n",
      "6972it [00:23, 195.87it/s]\u001b[A\n",
      "6992it [00:23, 192.09it/s]\u001b[A\n",
      "7014it [00:23, 199.78it/s]\u001b[A\n",
      "7035it [00:23, 193.38it/s]\u001b[A\n",
      "7055it [00:24, 185.49it/s]\u001b[A\n",
      "7074it [00:24, 185.41it/s]\u001b[A\n",
      "7094it [00:24, 187.31it/s]\u001b[A\n",
      "7113it [00:24, 178.39it/s]\u001b[A\n",
      "7131it [00:24, 170.69it/s]\u001b[A\n",
      "7151it [00:24, 177.34it/s]\u001b[A\n",
      "7170it [00:24, 180.43it/s]\u001b[A\n",
      "7189it [00:24, 182.45it/s]\u001b[A\n",
      "7209it [00:24, 183.33it/s]\u001b[A\n",
      "7229it [00:24, 184.50it/s]\u001b[A\n",
      "7250it [00:25, 188.23it/s]\u001b[A\n",
      "7269it [00:25, 180.69it/s]\u001b[A\n",
      "7288it [00:25, 173.82it/s]\u001b[A\n",
      "7306it [00:25, 170.12it/s]\u001b[A\n",
      "7324it [00:25, 167.36it/s]\u001b[A\n",
      "7342it [00:25, 168.60it/s]\u001b[A\n",
      "7361it [00:25, 174.31it/s]\u001b[A\n",
      "7381it [00:25, 180.32it/s]\u001b[A\n",
      "7400it [00:25, 177.00it/s]\u001b[A\n",
      "7418it [00:26, 167.42it/s]\u001b[A\n",
      "7436it [00:26, 167.74it/s]\u001b[A\n",
      "7453it [00:26, 159.51it/s]\u001b[A\n",
      "7471it [00:26, 163.37it/s]\u001b[A\n",
      "7488it [00:26, 161.13it/s]\u001b[A\n",
      "7505it [00:26, 161.39it/s]\u001b[A\n",
      "7524it [00:26, 165.70it/s]\u001b[A\n",
      "7542it [00:26, 168.27it/s]\u001b[A\n",
      "7560it [00:26, 170.33it/s]\u001b[A\n",
      "7578it [00:27, 172.86it/s]\u001b[A\n",
      "7596it [00:27, 171.59it/s]\u001b[A\n",
      "7614it [00:27, 169.07it/s]\u001b[A\n",
      "7632it [00:27, 170.93it/s]\u001b[A\n",
      "7650it [00:27, 172.15it/s]\u001b[A\n",
      "7670it [00:27, 177.76it/s]\u001b[A\n",
      "7691it [00:27, 182.81it/s]\u001b[A\n",
      "7710it [00:27, 182.39it/s]\u001b[A\n",
      "7729it [00:27, 177.69it/s]\u001b[A\n",
      "7747it [00:28, 176.20it/s]\u001b[A\n",
      "7765it [00:28, 175.99it/s]\u001b[A\n",
      "7783it [00:28, 172.55it/s]\u001b[A\n",
      "7801it [00:28, 169.42it/s]\u001b[A\n",
      "7819it [00:28, 172.10it/s]\u001b[A\n",
      "7837it [00:28, 163.58it/s]\u001b[A\n",
      "7858it [00:28, 174.47it/s]\u001b[A\n",
      "7876it [00:28, 162.79it/s]\u001b[A\n",
      "7893it [00:28, 164.40it/s]\u001b[A\n",
      "7910it [00:29, 157.43it/s]\u001b[A\n",
      "7926it [00:29, 157.55it/s]\u001b[A\n",
      "7942it [00:29, 156.32it/s]\u001b[A\n",
      "7959it [00:29, 159.07it/s]\u001b[A\n",
      "7976it [00:29, 160.98it/s]\u001b[A\n",
      "7993it [00:29, 161.48it/s]\u001b[A\n",
      "8010it [00:29, 161.20it/s]\u001b[A\n",
      "8029it [00:29, 166.34it/s]\u001b[A\n",
      "8046it [00:29, 159.20it/s]\u001b[A\n",
      "8065it [00:29, 167.62it/s]\u001b[A\n",
      "8083it [00:30, 169.74it/s]\u001b[A\n",
      "8101it [00:30, 163.79it/s]\u001b[A\n",
      "8119it [00:30, 167.82it/s]\u001b[A\n",
      "8137it [00:30, 169.93it/s]\u001b[A\n",
      "8155it [00:30, 166.65it/s]\u001b[A\n",
      "8173it [00:30, 169.18it/s]\u001b[A\n",
      "8190it [00:30, 167.61it/s]\u001b[A\n",
      "8211it [00:30, 178.41it/s]\u001b[A\n",
      "8229it [00:30, 170.41it/s]\u001b[A\n",
      "8247it [00:31, 160.48it/s]\u001b[A\n",
      "8264it [00:31, 161.16it/s]\u001b[A\n",
      "8284it [00:31, 171.92it/s]\u001b[A\n",
      "8304it [00:31, 177.86it/s]\u001b[A\n",
      "8322it [00:31, 173.59it/s]\u001b[A\n",
      "8340it [00:31, 173.99it/s]\u001b[A\n",
      "8359it [00:31, 177.35it/s]\u001b[A\n",
      "8378it [00:31, 179.51it/s]\u001b[A\n",
      "8398it [00:31, 181.51it/s]\u001b[A\n",
      "8417it [00:32, 177.09it/s]\u001b[A\n",
      "8435it [00:32, 171.68it/s]\u001b[A\n",
      "8453it [00:32, 164.60it/s]\u001b[A\n",
      "8470it [00:32, 159.20it/s]\u001b[A\n",
      "8489it [00:32, 166.29it/s]\u001b[A\n",
      "8506it [00:32, 162.22it/s]\u001b[A\n",
      "8524it [00:32, 166.46it/s]\u001b[A\n",
      "8541it [00:32, 166.76it/s]\u001b[A\n",
      "8558it [00:32, 164.06it/s]\u001b[A\n",
      "8575it [00:32, 162.94it/s]\u001b[A\n",
      "8592it [00:33, 159.49it/s]\u001b[A\n",
      "8610it [00:33, 162.74it/s]\u001b[A\n",
      "8628it [00:33, 164.87it/s]\u001b[A\n",
      "8645it [00:33, 163.86it/s]\u001b[A\n",
      "8663it [00:33, 166.20it/s]\u001b[A\n",
      "8683it [00:33, 174.42it/s]\u001b[A\n",
      "8701it [00:33, 170.11it/s]\u001b[A\n",
      "8719it [00:33, 167.25it/s]\u001b[A\n",
      "8736it [00:33, 160.77it/s]\u001b[A\n",
      "8753it [00:34, 162.09it/s]\u001b[A\n",
      "8771it [00:34, 164.11it/s]\u001b[A\n",
      "8792it [00:34, 173.60it/s]\u001b[A\n",
      "8810it [00:34, 166.57it/s]\u001b[A\n",
      "8827it [00:34, 166.87it/s]\u001b[A\n",
      "8846it [00:34, 172.18it/s]\u001b[A\n",
      "8864it [00:34, 172.38it/s]\u001b[A\n",
      "8882it [00:34, 161.17it/s]\u001b[A\n",
      "8899it [00:34, 161.29it/s]\u001b[A\n",
      "8916it [00:35, 159.18it/s]\u001b[A\n",
      "8932it [00:35, 158.95it/s]\u001b[A\n",
      "8948it [00:35, 158.72it/s]\u001b[A\n",
      "8968it [00:35, 169.24it/s]\u001b[A\n",
      "8985it [00:35, 165.72it/s]\u001b[A\n",
      "9002it [00:35, 163.05it/s]\u001b[A\n",
      "9021it [00:35, 169.64it/s]\u001b[A\n",
      "9040it [00:35, 175.42it/s]\u001b[A\n",
      "9060it [00:35, 179.48it/s]\u001b[A\n",
      "9078it [00:36, 167.77it/s]\u001b[A\n",
      "9097it [00:36, 170.05it/s]\u001b[A\n",
      "9117it [00:36, 178.37it/s]\u001b[A\n",
      "9135it [00:36, 168.31it/s]\u001b[A\n",
      "9153it [00:36, 165.15it/s]\u001b[A\n",
      "9170it [00:36, 165.58it/s]\u001b[A\n",
      "9192it [00:36, 179.39it/s]\u001b[A\n",
      "9211it [00:36, 175.41it/s]\u001b[A\n",
      "9229it [00:36, 165.41it/s]\u001b[A\n",
      "9246it [00:36, 165.77it/s]\u001b[A\n",
      "9263it [00:37, 163.40it/s]\u001b[A\n",
      "9280it [00:37, 164.92it/s]\u001b[A\n",
      "9297it [00:37, 165.76it/s]\u001b[A\n",
      "9314it [00:37, 166.03it/s]\u001b[A\n",
      "9331it [00:37, 166.82it/s]\u001b[A\n",
      "9348it [00:37, 163.93it/s]\u001b[A\n",
      "9366it [00:37, 167.66it/s]\u001b[A\n",
      "9384it [00:37, 170.14it/s]\u001b[A\n",
      "9402it [00:37, 169.94it/s]\u001b[A\n",
      "9420it [00:38, 162.13it/s]\u001b[A\n",
      "9440it [00:38, 172.04it/s]\u001b[A\n",
      "9458it [00:38, 161.50it/s]\u001b[A\n",
      "9477it [00:38, 167.35it/s]\u001b[A\n",
      "9496it [00:38, 171.87it/s]\u001b[A\n",
      "9514it [00:38, 173.45it/s]\u001b[A\n",
      "9532it [00:38, 172.74it/s]\u001b[A\n",
      "9550it [00:38, 167.05it/s]\u001b[A\n",
      "9567it [00:38, 164.77it/s]\u001b[A\n",
      "9590it [00:39, 245.40it/s]\u001b[A\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      " 17%|█▋        | 1/6 [00:00<00:01,  3.57it/s]\u001b[A\n",
      "100%|██████████| 6/6 [00:00<00:00, 13.85it/s]\u001b[A\n",
      "1it [00:39, 39.76s/it]\n"
     ]
    }
   ],
   "source": [
    "class SinglePassCluster():\n",
    "    \"\"\" Single-Pass Clustering\n",
    "    \"\"\"\n",
    "    def __init__(self, stopWords_path=\"\", my_stopwords=None,\n",
    "                 max_df=0.5, max_features=768,\n",
    "                 simi_threshold=0.5, cluster_name='', res_save_dir='', res_save_path=\"./cluster_res_ori.json\"):\n",
    "        self.simi_thr = simi_threshold\n",
    "        self.cluster_center_vec = []\n",
    "        self.cluster_vec_memory = []\n",
    "        self.idx_2_text = {}\n",
    "        self.cluster_2_idx = {}\n",
    "        self.res_path = res_save_path\n",
    "        self.cluster_name = cluster_name\n",
    "        self.res_save_dir = res_save_dir\n",
    "    \n",
    "    def load_SBERT_embeddings(self, embedding_path):\n",
    "        print('load in BERT embeddings...')\n",
    "        with open(embedding_path, 'rb') as handle:\n",
    "            pkl_data = pickle.load(handle)\n",
    "\n",
    "\n",
    "        np_data = []\n",
    "        if torch.is_tensor(pkl_data):\n",
    "            for val in tqdm(pkl_data):\n",
    "                np_data.append(val.numpy())\n",
    "        else:\n",
    "            for i, val in pkl_data.items():\n",
    "                np_data.append(val)\n",
    "        np_data = np.array(np_data)\n",
    "        print('np_data shape:{} np_data type:{}'.format(np_data.shape, type(np_data)))\n",
    "\n",
    "        return np_data\n",
    "\n",
    "    #def cosion_simi(self, vec):\n",
    "        #simi = cosine_similarity(np.array([vec]), np.array(self.cluster_center_vec))\n",
    "       # max_idx = np.argmax(simi, axis=1)[0]\n",
    "        #max_val = simi[0][max_idx]\n",
    "        #return max_val, max_idx\n",
    "\n",
    "    def cosion_simi(self, vec):\n",
    "        if np.array([vec]).shape[0]>5000:\n",
    "            batch_size = 5000  # 或根据你的内存限制来调整\n",
    "        else:\n",
    "            batch_size = np.array([vec]).shape[0] \n",
    "        max_simi, max_idx = 0, -1\n",
    "    # 分批处理以减少内存消耗\n",
    "        for start in range(0, len(self.cluster_center_vec), batch_size):\n",
    "            end = start + batch_size\n",
    "        # 计算当前批次的余弦相似度\n",
    "            simi = cosine_similarity(np.array([vec]), np.array(self.cluster_center_vec[start:end]))\n",
    "        # 在当前批次中找到最大相似度及其索引\n",
    "            local_max_idx = np.argmax(simi, axis=1)[0]\n",
    "            local_max_val = simi[0][local_max_idx]\n",
    "        # 如果当前批次的最大相似度大于之前的最大值，更新最大值和索引\n",
    "            if local_max_val > max_simi:\n",
    "                max_simi = local_max_val\n",
    "                max_idx = start + local_max_idx\n",
    "        return max_simi, max_idx\n",
    "\n",
    "    def single_pass(self, text_path, embedding_type, embedding_path=''):        \n",
    "        if embedding_type == 'SBERT':\n",
    "            SBERT_embeddings = self.load_SBERT_embeddings(embedding_path)\n",
    "            text_embeddings = SBERT_embeddings\n",
    "        else:\n",
    "            print('No match embedding type!')\n",
    "            exit()\n",
    "    \n",
    "        # Start loop\n",
    "        for idx, vec in tqdm(enumerate(text_embeddings)):\n",
    "            # Init the first cluster\n",
    "            if not self.cluster_center_vec:\n",
    "                self.cluster_center_vec.append(vec)\n",
    "                self.cluster_vec_memory.append([vec])\n",
    "                self.cluster_2_idx[0] = [idx]\n",
    "            # Clustering\n",
    "            else:\n",
    "                max_simi, max_idx = self.cosion_simi(vec)\n",
    "                if max_simi >= self.simi_thr:\n",
    "                    self.cluster_2_idx[max_idx].append(idx)\n",
    "                    \n",
    "                    # Update \n",
    "                    self.cluster_vec_memory[max_idx].append(vec)\n",
    "                    self.cluster_center_vec[max_idx] = np.mean(self.cluster_vec_memory[max_idx], axis=0)\n",
    "                else:\n",
    "                    self.cluster_center_vec.append(vec)\n",
    "                    self.cluster_2_idx[len(self.cluster_2_idx)] = [idx]\n",
    "\n",
    "                    self.cluster_vec_memory.append([vec])\n",
    "\n",
    "        with open(os.path.join(self.res_save_dir, self.cluster_name+'_ori.json'), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.cluster_2_idx, f, ensure_ascii=False)\n",
    "\n",
    "        #print(type(self.cluster_center_vec))\n",
    "        #print(type(self.cluster_center_vec[0]))\n",
    "\n",
    "        cluster_center_vec_list = [matrix.tolist() for matrix in self.cluster_center_vec]\n",
    "        with open(os.path.join(self.res_save_dir, self.cluster_name+'_centervec.json'), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cluster_center_vec_list, f, ensure_ascii=False)\n",
    "        \n",
    "        cluster_view_pool = res_process(self.cluster_2_idx, text_path)\n",
    "        pd.DataFrame(cluster_view_pool).to_json(os.path.join(self.res_save_dir, self.cluster_name+'_view.json'), indent=2, force_ascii=False, orient='records')\n",
    "        return pd.DataFrame(cluster_view_pool)\n",
    "\n",
    "\n",
    "def res_process(cluster_res, online_path):\n",
    "    \"\"\" Process The Results Into Analyzable Format\n",
    "    \"\"\"\n",
    "    online_df = pd.read_json(online_path, orient='records', lines=True)\n",
    "    #cluster_view_pool, cluster_res_pool = [], []\n",
    "    cluster_view_pool = []\n",
    "    for cluster_id, cluster_list in tqdm(cluster_res.items()):\n",
    "        count = len(cluster_list)\n",
    "        #view_contents, res_contents = [], []\n",
    "        view_contents = []\n",
    "        for id in cluster_list:\n",
    "            cur_data = online_df.iloc[id]\n",
    "            #id = cur_data['id']\n",
    "            #m = cur_data['month']\n",
    "            #s = cur_data['season']\n",
    "            #y = cur_data['year']\n",
    "            #y_m = str(cur_data['year']) + '_' + str(cur_data['month'])\n",
    "            #y_s = str(cur_data['year']) + '_' + str(cur_data['month'])\n",
    "            #label = cur_data['label']\n",
    "            content = cur_data['content']\n",
    "            cluster_label = cluster_id\n",
    "\n",
    "            #res_contents.append({\n",
    "                #'id': id,\n",
    "                #'year': y,\n",
    "                #'season': s,\n",
    "                #'month': m,\n",
    "                #'year-season': y_s,\n",
    "                #'year-month': y_m,\n",
    "                #'label': label,\n",
    "                #'cluster_label': cluster_label,\n",
    "                #'content': content\n",
    "            #})\n",
    "            view_contents.append(content)\n",
    "        cur_view_cluster = {\n",
    "            'cluster_id': cluster_id,\n",
    "            'count': count,\n",
    "            'contents': view_contents,\n",
    "        }\n",
    "        #cur_res_cluster = {\n",
    "            #'cluster_id': cluster_id,\n",
    "            #'count': count,\n",
    "            #'contents': res_contents\n",
    "        #}\n",
    "        cluster_view_pool.append(cur_view_cluster)\n",
    "        #cluster_res_pool.append(cur_res_cluster)\n",
    "\n",
    "    return cluster_view_pool#, cluster_res_pool\n",
    "\n",
    "def get_data_paths(args):\n",
    "\n",
    "    train_data_paths = args.train_data_path\n",
    "    dataset_name = args.data_name\n",
    "    print('dataset_name:', dataset_name)\n",
    "    user_save_dirs = args.user_save_dir\n",
    "    cluster_name = args.embedding_type + '_' + str(args.cluster_threshold)\n",
    "   \n",
    "    res = {\n",
    "        'data_dirs': './data/',\n",
    "        'train_data_paths': train_data_paths,\n",
    "        'dataset_name': dataset_name,\n",
    "        'user_save_dirs': user_save_dirs,\n",
    "        'cluster_name': cluster_name\n",
    "    }\n",
    "    print(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \"\"\" Topic Discovery Using Single-Pass Clustering\n",
    "    \"\"\"\n",
    "    print('========== single pass cluster ==========')\n",
    "    paths = get_data_paths(args)\n",
    "    dataset_name = args.data_name\n",
    "    online_train_path = args.train_data_path\n",
    "    user_save_dir = args.user_save_dir\n",
    "    SBERT_embeddings_path = args.SBERT_embeddings_path\n",
    "    embedding_type = args.embedding_type\n",
    "\n",
    "    threshold_list = [args.cluster_threshold]\n",
    "    print('='* 60)\n",
    "    print('='* 20, 'embedding_type:', embedding_type, '='*20)\n",
    "    cur_dir_path = os.path.join(user_save_dir, 'cluster_res', args.embedding_type)\n",
    "    \n",
    "    # ========== roll seasonal single pass ==========\n",
    "\n",
    "\n",
    "    if not os.path.exists(cur_dir_path):\n",
    "        os.makedirs(cur_dir_path)\n",
    "    for index, threshold in tqdm(enumerate(threshold_list)):\n",
    "        print('-'*10, 'threshold:', threshold, '-'*10)\n",
    "        cluster_name = embedding_type + '_' + str(threshold)\n",
    "\n",
    "        cluster = SinglePassCluster(max_features=100, simi_threshold=threshold, cluster_name=cluster_name, res_save_dir=cur_dir_path)\n",
    "        cluster.single_pass(online_train_path, embedding_type, SBERT_embeddings_path)\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    \"\"\" Main Entrance\n",
    "    \"\"\"\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data_name\", type=str, default= 'train')\n",
    "parser.add_argument(\"--embedding_type\", type=str, default='SBERT')\n",
    "parser.add_argument(\"--cluster_threshold\", type=float, default=0.78)\n",
    "\n",
    "parser.add_argument(\"--train_data_path\", type=str, default = './data/all_data.json')\n",
    "parser.add_argument(\"--user_save_dir\", type=str, default = './preprocessed/')\n",
    "parser.add_argument(\"--SBERT_embeddings_path\", type=str, default = './preprocessed/cls_embeddings.pkl')\n",
    "    \n",
    "parser.add_argument(\"--thres_low\", type=float, default=0)\n",
    "parser.add_argument(\"--thres_high\", type=float, default=float('inf'))\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "print(vars(args))\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88c3ddd7-25da-460b-9a22-51f1d2d8212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "cluster_result_path=\"./preprocessed/cluster_res/SBERT/SBERT_0.82_centervec.json\"\n",
    "#df = pd.read_json(cluster_result_path)\n",
    "with open(cluster_result_path, 'r') as file:\n",
    "    cluster_center_vec = json.load(file)\n",
    "cluster_center_vec=torch.tensor(cluster_center_vec, dtype=torch.float32)\n",
    "# 打印读取\n",
    "#print(cluster_center_vec.shape)\n",
    "cluster_result_path_v2=\"./preprocessed/cluster_res/SBERT/SBERT_0.82_ori.json\"\n",
    "#df = pd.read_json(cluster_result_path)\n",
    "with open(cluster_result_path_v2, 'r') as file:\n",
    "    cluster_2_idx_dict = json.load(file)\n",
    "# 打印读取\n",
    "cluster_2_idx = []\n",
    "for cluster_id, sample_ids in cluster_2_idx_dict.items():\n",
    "    # 将样本 ID 列表添加到二维列表中\n",
    "    cluster_2_idx.append(sample_ids)\n",
    "#print(cluster_2_idx[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e98fdab-3479-4c3b-9278-4ecbc021798b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3053, 3552, 376, 386, 65, 980, 413, 325, 1, 436, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "len_list = []\n",
    "for i in range(len(cluster_2_idx)):\n",
    "    len_list.append(len(cluster_2_idx[i]))\n",
    "    #print(len(cluster_2_idx[i]))\n",
    "\n",
    "#print(len(cluster_2_idx[12]))\n",
    "print(len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d2d7237-c262-40e8-afd1-bfb99ca3ba5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cluster_result_path_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e88b1ec-fbf3-492b-a74b-0a1c23b0b513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f3003ad-4a4c-47d2-9cf3-a1eda54777a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Cosine Similarities: 100%|██████████| 13/13 [00:00<00:00, 42.61it/s]\n",
      "Kernel Computing: 100%|██████████| 9590/9590 [00:02<00:00, 4203.71it/s]\n"
     ]
    }
   ],
   "source": [
    "#all vectors except idx tranformed into tensor\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import pickle\n",
    "from torch.autograd import Variable, Function\n",
    "from transformers import BertModel\n",
    "ZERO = 1e-8\n",
    "kernel_mu = np.arange(-1, 1.1, 0.1).tolist()\n",
    "kernel_sigma = [20 for _ in kernel_mu]\n",
    "kernel_mu.append(0.99)\n",
    "kernel_sigma.append(100)\n",
    "\n",
    "\n",
    "\n",
    "class Percept_Social_Event:\n",
    "    \n",
    "    def __init__(self, args, content, cluster_2_idx, cluster_center_vec, kernel_mu, kernel_sigma):\n",
    "    \n",
    "        self.cluster_center_vec = cluster_center_vec\n",
    "        self.cluster_2_idx = cluster_2_idx\n",
    "        self.content = content\n",
    "        self.args = args\n",
    "        self.cos_func = torch.nn.CosineSimilarity(dim=2)\n",
    "        self.cluster_drop = 0\n",
    "        for i in range(1, len(cluster_2_idx)):\n",
    "            if len(cluster_2_idx) < 5:\n",
    "                self.cluster_drop +=1\n",
    "        self.kernel_mu = kernel_mu # A list\n",
    "        self.kernel_sigma = kernel_sigma # A list\n",
    "        self.ZERO = args.ZERO\n",
    "        \n",
    "    def social_event_extraction (self, sim_feat_dims):\n",
    "\n",
    "        sim_feat = torch.empty((self.content.shape[0], sim_feat_dims - self.cluster_drop + 1))\n",
    "        samples_cluster_idx = torch.empty(self.content.shape[0], dtype=torch.long)\n",
    "        # 使用tqdm遍历每个聚类中心的索引\n",
    "        for i in tqdm(range(cluster_center_vec.shape[0]), desc=\"Computing Cosine Similarities\"):\n",
    "            samples_cluster_idx[self.cluster_2_idx[i]] = i\n",
    "            embeddings_current_cluster = content[self.cluster_2_idx[i]]\n",
    "            if embeddings_current_cluster.dim() == 1:\n",
    "                embeddings_current_cluster = embeddings_current_cluster.unsqueeze(0)\n",
    "\n",
    "            #print(embeddings_current_cluster.shape)\n",
    "            #print(self.cluster_center_vec.shape)\n",
    "            cos_sim_vector = self.cos_func(embeddings_current_cluster.unsqueeze(1), self.cluster_center_vec.clone().unsqueeze(0))\n",
    "            cos_sim_vector[:, [0, i]] = cos_sim_vector[:, [i, 0]]\n",
    "            lengths = torch.tensor([len(lst) for lst in self.cluster_2_idx])\n",
    "            lengths[0], lengths[i] = lengths[i], lengths[0]\n",
    "            columns_to_keep = lengths >= self.cluster_drop \n",
    "            columns_to_keep[0] = True\n",
    "            #print(i)\n",
    "            #print(cos_sim_vector.shape)\n",
    "            cos_sim_vector = torch.cat((cos_sim_vector[:, columns_to_keep],torch.zeros(cos_sim_vector.shape[0], 1)), dim = 1)\n",
    "            #print(len(self.cluster_2_idx[i]))\n",
    "            sim_feat[self.cluster_2_idx[i]] = cos_sim_vector[:, 0:(sim_feat_dims - self.cluster_drop + 1)]\n",
    "\n",
    "        Kernel_Features = torch.empty((self.content.shape[0],len(self.kernel_mu)))\n",
    "        Anti_Kernel_Features = torch.empty((self.content.shape[0],len(self.kernel_mu)))\n",
    "        self.kernel_mu = self.tensorize (self.kernel_mu)\n",
    "        self.kernel_sigma = self.tensorize (self.kernel_sigma)\n",
    "        for item_id in tqdm(range(self.content.shape[0]), desc = \"Kernel Computing\"):\n",
    "            Kernel_Features[item_id, :] = self.gaussian_kernel_pooling_2D(sim_feat[item_id, 0].unsqueeze(0))\n",
    "            Anti_Kernel_Features[item_id, :] = self.gaussian_kernel_pooling_2D(sim_feat[item_id][0:].unsqueeze(0))\n",
    "\n",
    "        Kernel_Features = torch.cat((Kernel_Features, Anti_Kernel_Features), dim=1)\n",
    "        cluster_center_feat = self.create_middle()\n",
    "        anti_cluster_center_feat = self.anti_create_middle()\n",
    "        #print(cluster_center_feat.shape)\n",
    "        #print(Kernel_Features.shape)\n",
    "        social_event_feat = torch.cat((Kernel_Features, cluster_center_feat, anti_cluster_center_feat), dim=1) #n,(2d+2k)\n",
    "        # 选取当前聚类中心\n",
    "        #center = cluster_center_vec[i].unsqueeze(0)  # 增加一个维度，以匹配cosine_similarity函数的输入要求\n",
    "        return social_event_feat\n",
    "    \n",
    "        \n",
    "    def create_middle(self):\n",
    "        content_avg = torch.zeros_like(self.content)\n",
    "        if not torch.is_tensor(self.cluster_center_vec):\n",
    "            self.cluster_center_vec = self.tensorize(self.cluster_center_vec)\n",
    "            \n",
    "        \n",
    "        if torch.is_tensor(self.cluster_2_idx):\n",
    "            print(\"cluster_2_idx is a tensor already\")\n",
    "            samples_cluster_idx = torch.empty(self.content.shape[0], dtype=torch.long)\n",
    "        else:\n",
    "            samples_cluster_idx = torch.empty(self.content.shape[0], dtype=torch.long)\n",
    "        \n",
    "        # 填充samples_cluster_idx，指示每个样本属于哪个聚类\n",
    "        for cluster_idx, sample_indices in enumerate(self.cluster_2_idx):\n",
    "            samples_cluster_idx[sample_indices] = cluster_idx\n",
    "\n",
    "        # 利用广播机制选择每个样本的聚类中心\n",
    "        content_avg = self.cluster_center_vec[samples_cluster_idx]\n",
    "        return content_avg\n",
    "\n",
    "    def anti_create_middle(self):\n",
    "        # 确保聚类中心向量是一个张量\n",
    "        if not torch.is_tensor(self.cluster_center_vec):\n",
    "            self.cluster_center_vec = self.tensorize(self.cluster_center_vec)\n",
    "\n",
    "        # 如果 cluster_2_idx 是二维列表，则转换为张量\n",
    "        if isinstance(self.cluster_2_idx, list):\n",
    "            # 假设每个子列表代表一个聚类中的样本索引\n",
    "            cluster_2_idx_temp = [torch.tensor(idx, dtype=torch.long) for idx in self.cluster_2_idx]\n",
    "    \n",
    "        num_clusters = self.cluster_center_vec.shape[0]\n",
    "        num_samples = self.content.shape[0]\n",
    "    \n",
    "        # 创建一个包含每个样本聚类索引的张量\n",
    "        samples_cluster_idx = torch.empty(num_samples, dtype=torch.long)\n",
    "        # 创建一个记录每个聚类的样本数的张量\n",
    "        cluster_sample_counts = torch.zeros(num_clusters, dtype=torch.long)\n",
    "\n",
    "        for cluster_idx, sample_indices in enumerate(cluster_2_idx_temp):\n",
    "            samples_cluster_idx[sample_indices] = cluster_idx\n",
    "            cluster_sample_counts[cluster_idx] = len(sample_indices)\n",
    "\n",
    "        # 为每个样本计算非所属聚类中心的条件平均值\n",
    "        content_non_member_avg = torch.zeros_like(self.content)\n",
    "    \n",
    "        # 计算每个样本的非成员聚类中心平均值\n",
    "        for j in range(num_samples):\n",
    "            own_cluster_idx = samples_cluster_idx[j]\n",
    "            own_cluster_center = self.cluster_center_vec[own_cluster_idx]\n",
    "\n",
    "            # 选择符合条件的聚类中心\n",
    "            valid_clusters = (cluster_sample_counts >= self.cluster_drop) & (torch.arange(num_clusters) != own_cluster_idx)\n",
    "            if valid_clusters.sum() > 0:\n",
    "                valid_cluster_centers = self.cluster_center_vec[valid_clusters]\n",
    "                non_member_avg = valid_cluster_centers.mean(dim=0)\n",
    "            else:\n",
    "                # 如果没有有效的其他聚类，使用原聚类中心作为默认值（或其他逻辑）\n",
    "                non_member_avg = own_cluster_center\n",
    "            content_non_member_avg[j] = non_member_avg\n",
    "        return content_non_member_avg\n",
    "\n",
    "    def tensorize(self, arr, dt=torch.float): #from a list to a tensor\n",
    "        if type(arr) == list and type(arr[0]) == torch.Tensor:\n",
    "            arr = torch.stack(arr)\n",
    "\n",
    "        return torch.as_tensor(arr, dtype=dt)\n",
    "\n",
    "    def gaussian_kernel_pooling(self, sim_values):\n",
    "        k, n = len(self.kernel_mu), len(sim_values)\n",
    "\n",
    "        if n == 0:\n",
    "            return self.tensorize(torch.zeros(k))\n",
    "\n",
    "        # (k) -> (n, k)\n",
    "        mu = self.kernel_mu.repeat(n, 1)\n",
    "        sigma = self.kernel_sigma.repeat(n, 1)\n",
    "\n",
    "        # (n) -> (k, n) -> (n, k)\n",
    "        if not torch.is_tensor(self.cluster_center_vec):\n",
    "            sim_values = self.tensorize(sim_values)\n",
    "        sim_values = sim_values.repeat(k, 1).T\n",
    "\n",
    "        # (n, k) -> (k)\n",
    "        kernel_features = torch.exp(-0.5 * ((sim_values - mu) * sigma)**2)\n",
    "        kernel_features = torch.sum(kernel_features, dim=0)\n",
    "\n",
    "        return self.normalize(kernel_features)\n",
    "\n",
    "    def normalize(self, kernel_features):\n",
    "        # Normalize\n",
    "        kernel_sum = torch.sum(kernel_features)\n",
    "        kernel_features /= (kernel_sum + ZERO)\n",
    "\n",
    "        return kernel_features\n",
    "\n",
    "    def normalize_2D(self, kernel_features): #normalization on each row\n",
    "        # Normalize each row\n",
    "        kernel_sum = torch.sum(kernel_features, dim=1, keepdim=True)  # Sum over columns for each row\n",
    "        kernel_features /= (kernel_sum + ZERO)  # Avoid division by zero\n",
    "\n",
    "        return kernel_features\n",
    "        \n",
    "    def gaussian_kernel_pooling_2D(self, sim_values):\n",
    "\n",
    "        if not torch.is_tensor(self.cluster_center_vec):\n",
    "            sim_values = self.tensorize(sim_values)\n",
    "        if sim_values.dim() == 1:\n",
    "            sim_values = sim_values.unsqueeze(0)\n",
    "        m, n = sim_values.shape  # m 是二维张量的行数, n 是列数\n",
    "        k = len(self.kernel_mu)\n",
    "\n",
    "        if n == 0:\n",
    "            return self.tensorize(torch.zeros((m, k)))\n",
    "\n",
    "        # (k) -> (1, k) -> (m, k)\n",
    "        mu = self.kernel_mu.unsqueeze(0).repeat(m, 1)\n",
    "        sigma = self.kernel_sigma.unsqueeze(0).repeat(m, 1)\n",
    "\n",
    "        # (m, n) -> (m, k, n)\n",
    "        sim_values = sim_values.unsqueeze(1).repeat(1, k, 1)\n",
    "        mu = mu.unsqueeze(2).repeat(1, 1, n)\n",
    "        sigma = sigma.unsqueeze(2).repeat(1, 1, n)\n",
    "\n",
    "        # (m, k, n) -> (m, k)\n",
    "        kernel_features = torch.exp(-0.5 * ((sim_values - mu) / sigma)**2)\n",
    "        kernel_features = torch.sum(kernel_features, dim=2)\n",
    "\n",
    "        return self.normalize_2D(kernel_features)\n",
    "\n",
    "def main(args, content, cluster_2_idx, cluster_center_vec, kernel_mu, kernel_sigma):\n",
    "    sim_feat_dims = len(cluster_2_idx)\n",
    "    social_event_extractor = Percept_Social_Event(args, content, cluster_2_idx, cluster_center_vec, kernel_mu, kernel_sigma)\n",
    "    social_event_feat = social_event_extractor.social_event_extraction(sim_feat_dims)\n",
    "    return social_event_feat\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Process kernel parameters.\")\n",
    "    \n",
    "    # 定义单个浮点数参数\n",
    "    parser.add_argument('--ZERO', type=float, default=1e-8, help='A small number to avoid division by zero.')\n",
    "    parser.add_argument('--Mode', type=str, default='2D', help='1D or 2D')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    return args\n",
    "\n",
    "\n",
    "#text_data_path = './data/train.json'\n",
    "#text_data = pd.read_json(text_data_path)\n",
    "#content = pd.DataFrame(text_data['content'])\n",
    "content = cls_embeddings\n",
    "args = parse_args()\n",
    "social_event_feat = main(args, content, cluster_2_idx, cluster_center_vec, kernel_mu, kernel_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fcd806-ddcb-43b6-8fed-acd108b4d662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7031337b-482b-4432-94d4-80bd4be14f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9590, 1580])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "social_event_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17d7fc3c-2754-4b91-9c56-5fc78aa659a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9590, 1580)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a12f8c2d-90c9-477e-9e8d-834749ac1295",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array = social_event_feat.numpy()\n",
    "numpy_array_train = numpy_array[:train_len, :]\n",
    "numpy_array_val = numpy_array[train_len:train_len+val_len, :]\n",
    "numpy_array_test = numpy_array[train_len+val_len:, :]\n",
    "# 保存为 .npy 文件\n",
    "np.save('./data/soc_feat_w_bert.npy', numpy_array)\n",
    "np.save('./data/soc_feat_w_bert_train.npy', numpy_array_train)\n",
    "np.save('./data/soc_feat_w_bert_val.npy', numpy_array_val)\n",
    "np.save('./data/soc_feat_w_bert_test.npy', numpy_array_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65593615-8232-44b1-8bfd-004bde301469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6713"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "679942e0-c18a-4c66-bdd7-917678d08b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = np.load('./data/soc_feat_w_bert.npy')\n",
    "\n",
    "# 将NumPy数组转换为PyTorch张量\n",
    "#social_event_feat = torch.from_numpy(numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24a73f29-c496-4435-9820-d217ad94b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar=arr[-9692:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3724988-6510-44ce-befe-9876ebc57997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9692, 1580)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bdcde4d-cb05-4685-9b11-42366aa325a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/soc_feat_w_bert_test.npy', ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8532068d-feed-475b-800b-363a8a0b38e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "numpy_array_train = np.load('./data/soc_feat_w_bert_train.npy')\n",
    "numpy_array_val = np.load('./data/soc_feat_w_bert_val.npy')\n",
    "numpy_array_test = np.load('./data/soc_feat_w_bert_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93ae08af-eaab-4f72-8093-3c48f0903173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1439, 1580)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_array_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d013d5-d0f3-4464-af47-84d8a883e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array_train = numpy_array_train[:-2000, :]\n",
    "#numpy_array_val = numpy_array_val[:-2000, :]\n",
    "#numpy_array_test = numpy_array_test[:-2000, :]\n",
    "np.save('./data/soc_feat_w_bert_train.npy', numpy_array_train)\n",
    "np.save('./data/soc_feat_w_bert_val.npy', numpy_array_val)\n",
    "np.save('./data/soc_feat_w_bert_test.npy', numpy_array_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a8ec5db-1b75-45ae-a1ea-38bbdff258e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31450, 1580)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_array_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b47eeac-bc0d-4a85-9f95-0fd281fd348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "file_path = './data/train.json'\n",
    "# 使用pandas的read_json函数读取JSON文件\n",
    "df_train = pd.read_json(file_path,orient='records', lines=True)\n",
    "df_train = df_train[:-2000]\n",
    "train_len = df_train.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e66687b-7042-40f9-bdd6-c401444acf50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31450"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a20318-babd-458b-a475-388ee1a8fff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_json('./data/train.json',orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
