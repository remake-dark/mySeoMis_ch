{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "463c0b6f-3d32-4898-8848-c1d24ecd9fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from local file: /root/.cache/OmniEvent_Model/s2s-mt5-ed model\n",
      "load from local file: /root/.cache/OmniEvent_Model/s2s-mt5-ed tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from local file: /root/.cache/OmniEvent_Model/s2s-mt5-eae model\n",
      "load from local file: /root/.cache/OmniEvent_Model/s2s-mt5-eae tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts: 100%|██████████| 20/20 [00:17<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from OmniEvent.infer import infer, get_pretrained\n",
    "import torch\n",
    "import sys\n",
    "import io\n",
    "\n",
    "# 全局变量来存储模型\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def efficient_infer(texts, task=\"EE\"):\n",
    "    global model\n",
    "    global tokenizer\n",
    "    if model is None:\n",
    "        ed_model, ed_tokenizer = get_pretrained(\"s2s-mt5-ed\", 'cuda')  # 加载模型\n",
    "        eae_model, eae_tokenizer = get_pretrained(\"s2s-mt5-eae\", 'cuda')\n",
    "        model = [ed_model, eae_model]\n",
    "        tokenizer = [ed_tokenizer, eae_tokenizer]\n",
    "    \n",
    "    # 创建一个临时的字符串流来捕获所有输出\n",
    "    temp_stdout = io.StringIO()\n",
    "    \n",
    "    results = []\n",
    "    for text in tqdm(texts, desc=\"Processing Texts\"):\n",
    "        # 保存当前的标准输出\n",
    "        old_stdout = sys.stdout\n",
    "        # 重定向标准输出到临时流\n",
    "        sys.stdout = temp_stdout\n",
    "        \n",
    "        # 调用infer函数，此时其内部的print调用不会输出到控制台\n",
    "        result = infer(text=text, model=model, tokenizer=tokenizer, task=task)\n",
    "        \n",
    "        # 恢复标准输出\n",
    "        sys.stdout = old_stdout\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    # 清空临时输出\n",
    "    temp_stdout.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "texts = [\n",
    "    \"2022年北京市举办了冬奥会。\",\n",
    "    \"今日有网友爆料：“在湖北襄阳，因家里起了点小争执...\",\n",
    "    \"据报道，昨日在上海一科技公司发生了数据泄露事件。美国总统在国会的一次演讲中提到了即将到来的经济改革。\",\n",
    "    \"美国总统在国会的一次演讲中提到了即将到来的经济改革。\",\n",
    "    \"近日，一起严重的森林火灾在加利福尼亚北部爆发。\",\n",
    "    \"昨天晚上，东京经历了数十年来最大的一次地震。\",\n",
    "    \"本周早些时候，一位名人在社交媒体上宣布了其即将发布的新书。\",\n",
    "    \"在巴黎举行的时装周上，多位设计师展示了他们的最新作品。\",\n",
    "    \"国际奥委会宣布将在2032年把夏季奥运会带到悉尼。\",\n",
    "    \"昨日，一名科学家团队在瑞士宣布了一项突破性的医学研究成果。\",\n",
    "    \"近日，网络安全问题再次引起了全球范围内的关注和讨论。\",\n",
    "    \"在昨晚的奖项典礼上，一位年轻音乐家获得了最佳新人奖。\",\n",
    "    \"有消息称，一家大型制药公司将投资数亿美元用于疾病研究。\",\n",
    "    \"教育部门宣布将增加资金支持远程教育项目。\",\n",
    "    \"国际环保组织在最新报告中强调了气候变化的严峻挑战。\",\n",
    "    \"警方昨日在多伦多市中心进行了一次大规模的毒品搜查行动。\",\n",
    "    \"一位著名电影导演在昨晚的访谈中透露了他的下一部电影计划。\",\n",
    "    \"最新市场研究报告显示，电动汽车销量在过去一年中大幅上涨。\",\n",
    "    \"昨天，一项关于全球经济趋势的研究在伦敦的一个国际会议上被发布。\",\n",
    "    \"在最近的一次科技展览上，一家初创公司展示了一种创新的人工智能应用。\"\n",
    "]\n",
    "results = efficient_infer(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d156d97-8688-4aa4-8da4-897f09d36f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "path = \"./data/train_data_en.csv\"\n",
    "train_data_en = pd.read_csv(path)\n",
    "def remove_urls(text):\n",
    "    # Regular expression to match URLs\n",
    "    pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "#Apply the function to each element in the 'content' column\n",
    "train_data_en['content'] = train_data_en['content'].apply(remove_urls)\n",
    "texts = train_data_en['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22260ca8-dee1-4a29-91e7-132e112debf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# 读取 .pkl 文件\n",
    "with open('./hy-tmp/val_new_cleaned.pkl', 'rb') as f:\n",
    "    train_data_en = pickle.load(f)\n",
    "def remove_urls(text):\n",
    "    # Regular expression to match URLs\n",
    "    pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    return re.sub(pattern, '', text)\n",
    "train_data_en['content'] = train_data_en['content'].apply(remove_urls)\n",
    "texts = train_data_en['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4682702-2a1d-4f00-bb70-6581f1f4b2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1654"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c18d9b6-0001-4f20-9258-3110aa917f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from local file: /root/.cache/OmniEvent_Model/s2s-mt5-ed model\n",
      "load from local file: /root/.cache/OmniEvent_Model/s2s-mt5-ed tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from local file: /root/.cache/OmniEvent_Model/s2s-mt5-eae model\n",
      "load from local file: /root/.cache/OmniEvent_Model/s2s-mt5-eae tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts: 100%|██████████| 1654/1654 [26:38<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "#尝试并行\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from OmniEvent.infer import infer, get_pretrained\n",
    "import pickle\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3,4,5'\n",
    "# 重新初始化 CUDA 运行时（PyTorch 示例，如果是 TensorFlow 可能需要重启 Kernel）\n",
    "torch.cuda.init()\n",
    "# 全局变量来存储模型和分词器\n",
    "models = None\n",
    "tokenizers = None\n",
    "\n",
    "def setup_model():\n",
    "    global models\n",
    "    global tokenizers\n",
    "    if models is None:\n",
    "        # 加载模型\n",
    "        ed_model, ed_tokenizer = get_pretrained(\"s2s-mt5-ed\", 'cuda')\n",
    "        eae_model, eae_tokenizer = get_pretrained(\"s2s-mt5-eae\", 'cuda')\n",
    "\n",
    "        # 确保模型在 CUDA 上\n",
    "        ed_model = ed_model.cuda()\n",
    "        eae_model = eae_model.cuda()\n",
    "        \n",
    "        # 使用 DataParallel 封装模型\n",
    "        ed_model = torch.nn.DataParallel(ed_model)\n",
    "        eae_model = torch.nn.DataParallel(eae_model)\n",
    "        \n",
    "        models = [ed_model, eae_model]\n",
    "        tokenizers = [ed_tokenizer, eae_tokenizer]\n",
    "\n",
    "def efficient_infer(texts, task=\"EE\"):\n",
    "    setup_model()\n",
    "    \n",
    "    # 创建一个临时的字符串流来捕获所有输出\n",
    "    temp_stdout = io.StringIO()\n",
    "    \n",
    "    results = []\n",
    "    for text in tqdm(texts, desc=\"Processing Texts\"):\n",
    "        # 保存当前的标准输出\n",
    "        old_stdout = sys.stdout\n",
    "        # 重定向标准输出到临时流\n",
    "        sys.stdout = temp_stdout\n",
    "        # 使用适当的模型和分词器\n",
    "        result = infer(text=text, model=[models[0].module, models[1].module], tokenizer=tokenizers, task=task, device = 'cuda')\n",
    "        \n",
    "        # 恢复标准输出\n",
    "        sys.stdout = old_stdout\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    # 清空临时输出\n",
    "    temp_stdout.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = efficient_infer(texts)\n",
    "with open('./fake_news_event_val', 'wb') as f:\n",
    "    pickle.dump(results , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afa55484-0e11-4ca5-aac2-14aec4cde85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1654"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1de54f3-5936-42e2-b259-b034fcabe064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('./en_fake_news_event_train', 'rb') as f:\n",
    "   # results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6538c04e-6c3c-49ac-a08e-6a210c8fa25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\"We would like to help prepare young people for their important roles in the future, as health emergencies and disease outbreaks may become even more common\"-@DrTedros #COVID19 #YouthDay \\xa0…',\n",
       " 'events': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[583][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "626d507f-6983-408b-8a8a-fb1bccfccdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 假设 result 是你提到的字典列表\n",
    "\n",
    "# 遍历列表中的每个字典\n",
    "for item in results:\n",
    "    # 使用 nltk 的 word_tokenize 方法分割单词\n",
    "    words = word_tokenize(item[0]['text'])\n",
    "    # 检查词的数量\n",
    "    if len(words) > 600:\n",
    "        # 如果词的数量大于500，截断到前500个词\n",
    "        item[0]['text'] = ' '.join(words[:600])\n",
    "\n",
    "# 打印修改后的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c079011-f32f-4740-a0a6-9394e8237ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 假设 result 是你提到的字典列表\n",
    "\n",
    "# 遍历列表中的每个字典\n",
    "for i in range(len(texts)):\n",
    "    # 使用 nltk 的 word_tokenize 方法分割单词\n",
    "    words = word_tokenize(texts[i])\n",
    "    # 检查词的数量\n",
    "    if len(words) > 600:\n",
    "        # 如果词的数量大于500，截断到前500个词\n",
    "        texts[i] = ' '.join(words[:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afb789da-0fba-4fb6-812e-7191869fd355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./en_fake_news_event_val', 'wb') as f:\n",
    "    pickle.dump(results , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2418188-a595-4887-855b-a9c7ea615730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Language Checking: 100%|██████████| 1654/1654 [00:00<00:00, 306800.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from translate import Translator\n",
    "from tqdm import tqdm\n",
    "def is_english(text):\n",
    "    \"\"\"\n",
    "    检查文本是否主要为英文。要求至少有2个英文字母且不含汉字。\n",
    "    \n",
    "    参数:\n",
    "    text (str): 需要检查的文本。\n",
    "    \n",
    "    返回:\n",
    "    bool: 如果文本主要为英文，则返回True，否则返回False。\n",
    "    \"\"\"\n",
    "    #english_count = sum(char.isalpha() for char in text)  # 计算文本中英文字母的数量\n",
    "    #contains_chinese = any('\\u4e00' <= char <= '\\u9fff' for char in text)  # 检查是否含有汉字\n",
    "    \n",
    "    return False #english_count >= 2 and not contains_chinese\n",
    "def format_event_extractions(data_list, sup_translator, sep=\" \"):\n",
    "    \"\"\"\n",
    "    根据给定的事件提取数据列表，生成格式化的字符串并用指定分隔符连接。\n",
    "    自动翻译英文为中文。\n",
    "    \"\"\"\n",
    "    if not data_list:\n",
    "        return \"\"\n",
    "\n",
    "    formatted_strings = []\n",
    "    \n",
    "    for data in data_list:\n",
    "        type_str = data['type']\n",
    "        # 如果type为英文，翻译为中文\n",
    "        if is_english(type_str):\n",
    "            type_str = sup_translator.translate(type_str)\n",
    "\n",
    "        formatted_str = \"[CLS] \"\n",
    "        formatted_str += f\"【类型】{type_str}【/类型】\"\n",
    "        for arg in data['arguments']:\n",
    "            mention = arg['mention']\n",
    "            role = arg['role']\n",
    "            # 如果mention为英文，翻译为中文\n",
    "            # 如果role为英文，翻译为中文\n",
    "            if is_english(role):\n",
    "                role = sup_translator.translate(role)\n",
    "            formatted_str += f\"【{role}】{mention}【/{role}】\"\n",
    "        trigger = data['trigger']\n",
    "        formatted_str += f\"【触发词】{trigger}【/触发词】。 [SEP]\"\n",
    "        formatted_strings.append(formatted_str)\n",
    "\n",
    "    return sep.join(formatted_strings)\n",
    "\n",
    "def format_event_extractions_en(data_list, sep=\" \"):\n",
    "    \"\"\"\n",
    "    Generate formatted strings based on a given list of event extraction data,\n",
    "    and concatenate them using the specified separator. This function is tailored\n",
    "    for handling English event data.\n",
    "    \"\"\"\n",
    "    if not data_list:\n",
    "        return \"\"\n",
    "\n",
    "    formatted_strings = []\n",
    "    \n",
    "    for data in data_list:\n",
    "        type_str = data['type']\n",
    "\n",
    "        # Initialize the formatted string with [CLS] for starting the sequence\n",
    "        formatted_str = \"[CLS] \"\n",
    "        formatted_str += f\"[Type] {type_str} [/Type]\"\n",
    "\n",
    "        # Process each argument in the event\n",
    "        for arg in data['arguments']:\n",
    "            mention = arg['mention']\n",
    "            role = arg['role']\n",
    "\n",
    "            formatted_str += f\"[{role}] {mention} [/{role}]\"\n",
    "\n",
    "        # Append the trigger word at the end\n",
    "        trigger = data['trigger']\n",
    "        formatted_str += f\"[Trigger] {trigger} [/Trigger]. [SEP]\"\n",
    "        formatted_strings.append(formatted_str)\n",
    "\n",
    "    return sep.join(formatted_strings)\n",
    "\n",
    "def process_event_data(event_data, sup_translator, mode, sep=\" \"):\n",
    "    \"\"\"\n",
    "    处理包含多个样本的事件数据，每个样本包含一个事件列表。\n",
    "\n",
    "    参数:\n",
    "    event_data (list): 包含事件数据的大列表，其中每个元素是一个字典，带有一个'events'键。\n",
    "    sep (str): 用于连接事件格式化字符串的分隔符。\n",
    "\n",
    "    返回:\n",
    "    list: 每个样本处理后得到的格式化字符串列表。\n",
    "    \"\"\"\n",
    "    formatted_results = []\n",
    "    if mode == \"en\":\n",
    "    # 遍历大数据集中的每个样本\n",
    "        for sample in tqdm(event_data, desc=\"Language Checking\"):\n",
    "        # 获取每个样本中的事件列表\n",
    "            events = sample[0]['events']\n",
    "        # 格式化当前样本的事件列表\n",
    "            formatted_text = format_event_extractions_en(events, sep=sep)\n",
    "        # 将格式化后的文本添加到结果列表\n",
    "            formatted_results.append(formatted_text)\n",
    "    else: \n",
    "        for sample in tqdm(event_data, desc=\"Language Checking\"):\n",
    "        # 获取每个样本中的事件列表\n",
    "            events = sample[0]['events']\n",
    "        # 格式化当前样本的事件列表\n",
    "            formatted_text = format_event_extractions(events, sup_translator, sep=sep)\n",
    "        # 将格式化后的文本添加到结果列表\n",
    "            formatted_results.append(formatted_text)\n",
    "    formatted_results = [[item] for item in formatted_results]\n",
    "    return formatted_results\n",
    "\n",
    "# 示例使用\n",
    "translator = Translator(from_lang='en', to_lang='zh')\n",
    "event_data = results\n",
    "mode = 'en' #英文\n",
    "# 调用函数并打印结果\n",
    "results_data = process_event_data(event_data, translator, mode, sep=\" || \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193dcd71-1e24-46bd-8a06-a2aae69bfd98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "452fb8ce-c52b-4504-8be5-0d0f2487d34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'str': '飞机', 'hit': [7, 2, 7, 2], 'type': {'name': 'basic.tool.physical', 'i18n': '工具', 'flag': 2, 'path': '/product.generic/'}, 'meaning': {'related': ['遥控玩具', '拼图玩具', '智力拼装玩具', '桌面玩具', '木质玩具', '仿真模型', '电子电动玩具', '木制纸品玩具', '布毛绒塑胶玩具', '积木玩具']}, 'tag': 'basic.tool.physical', 'tag_i18n': '工具'}, {'str': '马来西亚', 'hit': [28, 4, 27, 4], 'type': {'name': 'loc.country_region', 'i18n': '国家或地区', 'flag': 1, 'path': '/loc.generic/loc.geo/loc.geo.district/loc.geo.populated_place/'}, 'meaning': {'related': ['莱索托王国', '马拉维共和国', '塞拉利昂共和国', '斯威士兰王国', '冈比亚共和国', '塞舌尔共和国', '乌干达共和国', '坦桑尼亚联合共和国', '莫桑比克共和国', '加纳共和国']}, 'tag': 'loc.country_region', 'tag_i18n': '国家或地区'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "obj = {\"str\": \"到底发生了什么飞机要绕回来？[泪] 所以是军方通报了之后马来西亚才扩大搜救范围的吗？。\"}\n",
    "req_str = json.dumps(obj).encode()\n",
    "\n",
    "url = \"https://texsmart.qq.com/api\"\n",
    "r = requests.post(url, data=req_str)\n",
    "r.encoding = \"utf-8\"\n",
    "#print(r.text)\n",
    "res = json.loads(r.text)['entity_list']\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8494b021-d95d-4f8b-99c8-ea93d5c0dad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Texts: 100%|██████████| 1654/1654 [10:31<00:00,  2.62it/s]\n",
      "Formatting Entities: 100%|██████████| 1654/1654 [00:00<00:00, 30823.07it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm  # 导入tqdm\n",
    "\n",
    "# 函数化请求过程，以便可以处理多个文本\n",
    "def call_texsmart_api(texts):\n",
    "    url = \"https://texsmart.qq.com/api\"\n",
    "    results = []\n",
    "    \n",
    "    # 遍历所有文本，并使用tqdm显示进度条\n",
    "    for text in tqdm(texts, desc=\"Processing Texts\"):\n",
    "        obj = {\"str\": text}\n",
    "        req_str = json.dumps(obj).encode()  # 编码请求数据\n",
    "        response = requests.post(url, data=req_str)  # 发送请求\n",
    "        response.encoding = \"utf-8\"  # 设置响应编码\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            res = json.loads(response.text)['entity_list']  # 解析响应\n",
    "            results.append(res)  # 添加解析结果到列表\n",
    "        else:\n",
    "            kk = None\n",
    "            print(\"Failed to process text:\")\n",
    "            results.append(kk)  # 处理失败时添加None\n",
    "    \n",
    "    return results\n",
    "\n",
    "def extract_and_join_strings(entities, sep=' [SEP] '):\n",
    "    \"\"\"\n",
    "    提取entities中每个字典的'str'字段并用指定的分隔符连接成一个字符串。\n",
    "\n",
    "    参数:\n",
    "    entities (list): 包含实体字典的列表。\n",
    "    sep (str): 用于连接字符串的分隔符。\n",
    "\n",
    "    返回:\n",
    "    str: 所有'str'字段连接后的字符串。\n",
    "    \"\"\"\n",
    "    # 使用列表推导式提取每个实体中的'str'字段\n",
    "    strings = [entity['str'] for entity in entities if 'str' in entity]\n",
    "    \n",
    "    # 使用sep连接提取出的字符串\n",
    "    return sep.join(strings)\n",
    "\n",
    "def process_entity_lists(entity_lists, sep=' [SEP] '):\n",
    "    \"\"\"\n",
    "    处理包含多个实体列表的集合，为每个列表生成格式化的字符串，每个字符串作为单独的列表项返回，并显示进度条。\n",
    "\n",
    "    参数:\n",
    "    entity_lists (list): 包含多个实体列表的列表。\n",
    "    sep (str): 用于连接字符串的分隔符。\n",
    "\n",
    "    返回:\n",
    "    list: 每个实体列表处理后得到的包含一个字符串的列表。\n",
    "    \"\"\"\n",
    "    formatted_results = []\n",
    "    \n",
    "    # 使用 tqdm 进行循环，显示进度条\n",
    "    for entities in tqdm(entity_lists, desc=\"Formatting Entities\"):\n",
    "        # 格式化当前实体列表并添加到结果列表，每个结果作为一个单元素列表\n",
    "        if entities is not None:\n",
    "            formatted_text = extract_and_join_strings(entities, sep=sep)\n",
    "        else:\n",
    "            formatted_text = ''\n",
    "        formatted_results.append([formatted_text])\n",
    "    return formatted_results\n",
    "    \n",
    "# 示例使用\n",
    "# 调用函数处理多个文本\n",
    "entity_lists = call_texsmart_api(texts)\n",
    "result_entities = process_entity_lists(entity_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2ac3552a-0444-4213-b01c-e73efcfbde2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1652"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "89848ee1-83fa-401f-a682-e414c555c8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging Events: 100%|██████████| 1652/1652 [00:00<00:00, 350444.58it/s]\n"
     ]
    }
   ],
   "source": [
    "#合并\n",
    "\n",
    "def merge_event_data(results_data, result_entities):\n",
    "    \"\"\"\n",
    "    将两个二维列表中的字符串进行合并，同时显示进度条。\n",
    "\n",
    "    参数:\n",
    "    results_data (list): 包含字符串列表的二维列表。\n",
    "    result_entities (list): 包含字符串列表的二维列表。\n",
    "\n",
    "    返回:\n",
    "    list: 合并后的二维列表，其中每个元素是合并后的字符串。\n",
    "    \"\"\"\n",
    "    # 合并两个列表中的字符串，使用tqdm显示进度条\n",
    "    results_event = [[data[0] + entities[0]] for data, entities in tqdm(zip(results_data, result_entities), total=len(results_data), desc=\"Merging Events\")]\n",
    "    for i in range(len(results_event)):\n",
    "        if results_event[i] == ['']:  # 检查是否为空字符串的列表\n",
    "            results_event[i] = [' [SEP] '] \n",
    "    return results_event\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 调用函数并打印结果\n",
    "results_event = merge_event_data(results_data, result_entities)\n",
    "\n",
    "\n",
    "\n",
    "texts_dataset = train_data_en\n",
    "texts_dataset['event'] = results_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "592753ff-eba1-4941-8dd9-35389f46cb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To find longevity as an artist, one must have ...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>0</td>\n",
       "      <td>[[CLS] [Type] attack [/Type][entity] Justin [/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who is the next Bachelorette 2018? ...it's Bec...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>0</td>\n",
       "      <td>[[CLS] [Type] startposition [/Type][position] ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It’s Gilmore Girls meets Dawson’s Creek on Dis...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>0</td>\n",
       "      <td>[[CLS] [Type] transport [/Type][origin] Transy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In actuality, it's unlikely that their bliss i...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>1</td>\n",
       "      <td>[[CLS] [Type] marry [/Type][person] their [/pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Excerpt) Read more at: E! Online\\n\\nWake Up T...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>0</td>\n",
       "      <td>[[CLS] [Type] meet [/Type][entity] you [/entit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\nPrincess Diana in 1995 with her sons, Prin...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>0</td>\n",
       "      <td>[[CLS] [Type] die [/Type][person] Harry [/pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Biggest Loser trainer and TODAY show health co...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>0</td>\n",
       "      <td>[[CLS] [Type] die [/Type][entity] TODAY [/enti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Don't mess with Lupita Nyong'o's hair!\\n\\nOn T...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>0</td>\n",
       "      <td>[[CLS] [Type] broadcast [/Type][entity] Nyong'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Brie Bella is revealing her struggles over whe...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>0</td>\n",
       "      <td>[[CLS] [Type] transferownership [/Type][entity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Get the latest from TODAY Sign up for our news...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>0</td>\n",
       "      <td>[[CLS] [Type] beborn [/Type][Trigger] birth [/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>As expected, The New York Times’ exposé on the...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>0</td>\n",
       "      <td>[[CLS] [Type] broadcast [/Type][time] Monday [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>While we definitely don't envy anyone who has ...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>0</td>\n",
       "      <td>[trousers [SEP] summer [SEP] days [SEP] pair o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Kim Kardashian (L) and Khloe Kardashian attend...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>0</td>\n",
       "      <td>[[CLS] [Type] beborn [/Type][person] husband [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nikki Bella Carps About Fiancé John Cena While...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>0</td>\n",
       "      <td>[[CLS] [Type] transport [/Type][person] Her [/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sam Smith is done with hiding. In a new interv...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>0</td>\n",
       "      <td>[[CLS] [Type] meet [/Type][Trigger] interview ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sometimes beauty means pain! Nazanin Mandi is ...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>0</td>\n",
       "      <td>[[CLS] [Type] transport [/Type][agent] she [/a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Retail therapy! Selena Gomez stepped out in Ne...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>0</td>\n",
       "      <td>[[CLS] [Type] meet [/Type][Trigger] engagement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\\n\\n\\n\\nDon’t go, Dr. Ben Warren! Another Grey...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>0</td>\n",
       "      <td>[[CLS] [Type] endposition [/Type][time] Septem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>English-American actress\\n\\nEmily Olivia Leah ...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>0</td>\n",
       "      <td>[[CLS] [Type] nominate [/Type][place] Dorking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Selena Gomez and Justin Bieber are going stron...</td>\n",
       "      <td>gossipcop</td>\n",
       "      <td>0</td>\n",
       "      <td>[[CLS] [Type] transport [/Type][Trigger] jette...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content   category label   \n",
       "0   To find longevity as an artist, one must have ...  gossipcop     0  \\\n",
       "1   Who is the next Bachelorette 2018? ...it's Bec...  gossipcop     0   \n",
       "2   It’s Gilmore Girls meets Dawson’s Creek on Dis...  gossipcop     0   \n",
       "3   In actuality, it's unlikely that their bliss i...  gossipcop     1   \n",
       "4   (Excerpt) Read more at: E! Online\\n\\nWake Up T...  gossipcop     0   \n",
       "5   \\n\\nPrincess Diana in 1995 with her sons, Prin...  gossipcop     0   \n",
       "6   Biggest Loser trainer and TODAY show health co...  gossipcop     0   \n",
       "7   Don't mess with Lupita Nyong'o's hair!\\n\\nOn T...  gossipcop     0   \n",
       "8   Brie Bella is revealing her struggles over whe...  gossipcop     0   \n",
       "9   Get the latest from TODAY Sign up for our news...  gossipcop     0   \n",
       "10  As expected, The New York Times’ exposé on the...  gossipcop     0   \n",
       "11  While we definitely don't envy anyone who has ...  gossipcop     0   \n",
       "12  Kim Kardashian (L) and Khloe Kardashian attend...  gossipcop     0   \n",
       "13  Nikki Bella Carps About Fiancé John Cena While...  gossipcop     0   \n",
       "14  Sam Smith is done with hiding. In a new interv...  gossipcop     0   \n",
       "15  Sometimes beauty means pain! Nazanin Mandi is ...  gossipcop     0   \n",
       "16  Retail therapy! Selena Gomez stepped out in Ne...  gossipcop     0   \n",
       "17  \\n\\n\\n\\nDon’t go, Dr. Ben Warren! Another Grey...  gossipcop     0   \n",
       "18  English-American actress\\n\\nEmily Olivia Leah ...  gossipcop     0   \n",
       "19  Selena Gomez and Justin Bieber are going stron...  gossipcop     0   \n",
       "\n",
       "                                                event  \n",
       "0   [[CLS] [Type] attack [/Type][entity] Justin [/...  \n",
       "1   [[CLS] [Type] startposition [/Type][position] ...  \n",
       "2   [[CLS] [Type] transport [/Type][origin] Transy...  \n",
       "3   [[CLS] [Type] marry [/Type][person] their [/pe...  \n",
       "4   [[CLS] [Type] meet [/Type][entity] you [/entit...  \n",
       "5   [[CLS] [Type] die [/Type][person] Harry [/pers...  \n",
       "6   [[CLS] [Type] die [/Type][entity] TODAY [/enti...  \n",
       "7   [[CLS] [Type] broadcast [/Type][entity] Nyong'...  \n",
       "8   [[CLS] [Type] transferownership [/Type][entity...  \n",
       "9   [[CLS] [Type] beborn [/Type][Trigger] birth [/...  \n",
       "10  [[CLS] [Type] broadcast [/Type][time] Monday [...  \n",
       "11  [trousers [SEP] summer [SEP] days [SEP] pair o...  \n",
       "12  [[CLS] [Type] beborn [/Type][person] husband [...  \n",
       "13  [[CLS] [Type] transport [/Type][person] Her [/...  \n",
       "14  [[CLS] [Type] meet [/Type][Trigger] interview ...  \n",
       "15  [[CLS] [Type] transport [/Type][agent] she [/a...  \n",
       "16  [[CLS] [Type] meet [/Type][Trigger] engagement...  \n",
       "17  [[CLS] [Type] endposition [/Type][time] Septem...  \n",
       "18  [[CLS] [Type] nominate [/Type][place] Dorking ...  \n",
       "19  [[CLS] [Type] transport [/Type][Trigger] jette...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6340d7f8-c2a9-44ff-9b6d-17608d935dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_dataset.to_csv('/en_fake_news_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76dde9-948b-4a1d-be71-5fedf74adc85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955aeec5-cbb9-4e25-b344-e7144c8e2416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd181c8d-faa5-4cc9-99e4-3347b4d11849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7465687f-5960-4e24-af4d-d0f4d442c3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66834feb-9aae-4692-acf6-516cdba5f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "def extract_and_join_strings(entities, sep=\"SEP\"):\n",
    "    \"\"\"\n",
    "    提取entities中每个字典的'str'字段并用指定的分隔符连接成一个字符串。\n",
    "\n",
    "    参数:\n",
    "    entities (list): 包含实体字典的列表。\n",
    "    sep (str): 用于连接字符串的分隔符。\n",
    "\n",
    "    返回:\n",
    "    str: 所有'str'字段连接后的字符串。\n",
    "    \"\"\"\n",
    "    # 使用列表推导式提取每个实体中的'str'字段\n",
    "    strings = [entity['str'] for entity in entities if 'str' in entity]\n",
    "    \n",
    "    # 使用sep连接提取出的字符串\n",
    "    return sep.join(strings)\n",
    "joined_strings = extract_and_join_strings(entities)\n",
    "print(joined_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9be82957-5e06-4c5a-9660-1f2b16238f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "def extract_strings_from_dicts(dict_list):\n",
    "    \"\"\"\n",
    "    从字典列表中提取 'str' 键的值并组装成列表。\n",
    "\n",
    "    参数:\n",
    "    dict_list (list): 包含字典的列表。\n",
    "\n",
    "    返回:\n",
    "    list: 包含所有 'str' 值的列表。\n",
    "    \"\"\"\n",
    "    # 初始化结果列表\n",
    "    result_list = []\n",
    "\n",
    "    # 遍历列表中的每个字典\n",
    "    for item in dict_list:\n",
    "        # 检查 'str' 键是否在字典中\n",
    "        if 'str' in item:\n",
    "            # 添加 'str' 键的值到结果列表\n",
    "            result_list.append(item['str'])\n",
    "\n",
    "    # 返回结果列表\n",
    "    return result_list\n",
    "result_list = extract_strings_from_dicts(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04cfdb33-e509-466a-9efc-56ee34bb7217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['飞机', '马来西亚']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b73cf6-308b-4d35-a564-dde8e603e481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
